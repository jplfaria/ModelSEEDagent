{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebb188-3a4f-47fa-879e-b9ddffd17f8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Apple Silicon GPU) available: True\n",
      "CUDA (NVIDIA GPU) available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config.settings import load_config\n",
    "from src.llm import LLMFactory\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Print available devices\n",
    "print(f\"MPS (Apple Silicon GPU) available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"CUDA (NVIDIA GPU) available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Create local LLM config\n",
    "local_llm_config = {\n",
    "    \"llm_name\": \"llama-3.1-8b\",\n",
    "    \"system_content\": config.local.system_content,\n",
    "    \"path\": \"/Users/jplfaria/.llama/checkpoints/Llama3.1-8B\",\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Create LLM and test\n",
    "llm = LLMFactory.create(\"local\", local_llm_config)\n",
    "\n",
    "try:\n",
    "    response = llm.predict(\"Test message: What is your name?\")\n",
    "    print(f\"Test successful. Response from LocalLLM:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Error testing LocalLLM:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc84b0d-dbb0-4b81-b2ed-7072a747c9f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Apple Silicon GPU) available: True\n",
      "CUDA (NVIDIA GPU) available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config.settings import load_config\n",
    "from src.llm import LLMFactory\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Create local LLM config for Hugging Face model\n",
    "local_llm_config = {\n",
    "    \"llm_name\": \"llama-3.1-8b\",\n",
    "    \"system_content\": config.local.system_content,\n",
    "    \"path\": \"/Users/jplfaria/.llama/checkpoints/Llama3.1-8B-HuggingFace/models--meta-llama--Llama-3.1-8B\",\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Create LLM and test\n",
    "llm = LLMFactory.create(\"local\", local_llm_config)\n",
    "\n",
    "try:\n",
    "    prompt = \"Explain metabolic modeling in simple terms\"\n",
    "    print(f\"\\nGenerating response for: {prompt}\")\n",
    "    response = llm.predict(prompt)\n",
    "    print(f\"\\nResponse: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452f425f-7dc0-4431-911a-93c52c59a02e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cd469b5b3840d4a254de1aca61eda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7f9950d3984bb7bf2ee5cac313c0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b965e000f82446708ea8c5201e38af86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5a6bc4409848c19bd5fc18bf5a88a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4d726814254551b2399f980a9892b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d0a13a9fca46cda93c9f6659ac1fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e515e7a9729f4bf4be26fec571ccd35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c929d0e29f2e42908ae9fc8cb6c5a6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d4986c14094aef873c4d4ed145a2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e5c296947a43879784e21d31644040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f920d8fd0247eeac0798f0c04e16c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c635c8f87914b4ca857e6daaf019992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/Users/jplfaria/.llama/checkpoints/Llama3.1-8B-HuggingFace\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/Users/jplfaria/.llama/checkpoints/Llama3.1-8B-HuggingFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bdbc88-389e-4595-b580-47ac11a813bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaf753dd04a4a38b724faa58210270d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa37dd15dda4b65986b251150dcd80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f4a82d39a44371b8a98c038277721e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3179c9ecad7d4583b77a31537b17b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ff1abadc0b4d1783638256f50193a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ffe32814f249e293c9b3b33adf99d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406afce4076a434cbc95012d6c814ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea99e56b6a77460fbb8fef215d19cb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81dffa065ad4f6b9e525ca09b8ac01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cecf981b0b47eea1fd18845b4075cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "cache_dir = \"/Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28396b-3428-4713-800a-a01db115079f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## testing hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ccd1ed-9aae-4396-a782-24d5412f0150",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of model directory:\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B\n",
      "Files: []\n",
      "Subdirectories: ['snapshots', '.no_exist', 'blobs', 'refs']\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/snapshots\n",
      "Files: []\n",
      "Subdirectories: ['13afe5124825b4f3751f836b40dafda64c1ed062']\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062\n",
      "Files: ['tokenizer_config.json', 'special_tokens_map.json', 'model-00001-of-00002.safetensors', 'config.json', 'tokenizer.json', 'generation_config.json', 'model-00002-of-00002.safetensors', 'model.safetensors.index.json']\n",
      "Subdirectories: []\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/.no_exist\n",
      "Files: []\n",
      "Subdirectories: ['13afe5124825b4f3751f836b40dafda64c1ed062']\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/.no_exist/13afe5124825b4f3751f836b40dafda64c1ed062\n",
      "Files: ['model.safetensors', 'added_tokens.json', 'tokenizer.model']\n",
      "Subdirectories: []\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/blobs\n",
      "Files: ['d3a1f0f5f401eeadca0c7a6786bd9e877fd42e58', 'cfabacc2620186cd3dd4b1dde9a37e057208636e', '5cc5f00a5b203e90a27a3bd60d1ec393b07971e8', '584d8d3e3f82f7964955174dfe5e3b1cf117a9d859f022cfdf7fcb884856e002', '47d4a5aa69cdef91a53b77f5c5583647a578ca0e', 'cb9ec25536e44d86778b10509d3e5bdca459a5cf', '2d73a6863086ff9d491c28e49df9fb697cd92c2b', '4719a04514ec2f060240711b7c33ab21187cac730ecaba3040b7a0fd95a9cefb']\n",
      "Subdirectories: []\n",
      "\n",
      "Directory: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/refs\n",
      "Files: ['main']\n",
      "Subdirectories: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "huggingface_path = \"/Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B\"\n",
    "print(\"Contents of model directory:\")\n",
    "for root, dirs, files in os.walk(huggingface_path):\n",
    "    print(f\"\\nDirectory: {root}\")\n",
    "    print(\"Files:\", files)\n",
    "    print(\"Subdirectories:\", dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78347202-72fe-46cb-8bc3-104972e507d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config.settings import load_config\n",
    "from src.llm import LLMFactory\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Get the snapshot path\n",
    "base_path = \"/Users/jplfaria/.llama/checkpoints/Llama3.1-8B-HuggingFace/models--meta-llama--Llama-3.1-8B\"\n",
    "snapshot_path = f\"{base_path}/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n",
    "\n",
    "# Create local LLM config with longer max_tokens\n",
    "local_llm_config = {\n",
    "    \"llm_name\": \"llama-3.1-8b\",\n",
    "    \"system_content\": config.local.system_content,\n",
    "    \"path\": snapshot_path,\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"max_tokens\": 5000,  # Increased from 100\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(f\"\\nLoading model from: {snapshot_path}\")\n",
    "\n",
    "# Create LLM and test\n",
    "llm = LLMFactory.create(\"local\", local_llm_config)\n",
    "\n",
    "try:\n",
    "    prompts = [\n",
    "        \"Who are you?\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        response = llm.predict(prompt)\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        print(\"-\" * 80)  # Separator between responses\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up (optional but good practice)\n",
    "    if 'llm' in locals():\n",
    "        del llm\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f43c65-5bcb-4018-b732-bfffda1628a9",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ce6bee-339e-4e0a-b540-53585301b0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from: /Users/jplfaria/.llama/checkpoints/Llama3.1-8B-HuggingFace/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ead2b72ca94e978b39d911078440a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config.settings import load_config\n",
    "from src.llm import LLMFactory\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Get the snapshot path\n",
    "base_path = \"/Users/jplfaria/.llama/checkpoints/Llama3.1-8B-HuggingFace/models--meta-llama--Llama-3.1-8B\"\n",
    "snapshot_path = f\"{base_path}/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n",
    "\n",
    "# Create local LLM config with longer max_tokens\n",
    "local_llm_config = {\n",
    "    \"llm_name\": \"llama-3.1-8b\",\n",
    "    \"system_content\": config.local.system_content,\n",
    "    \"path\": snapshot_path,\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"max_tokens\": 5000,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(f\"\\nLoading model from: {snapshot_path}\")\n",
    "# Create LLM and test\n",
    "llm = LLMFactory.create(\"local\", local_llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398554ab-5adb-4a49-9535-f3d65514fe55",
   "metadata": {},
   "source": [
    "### Inference cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d7eb4-f43a-499f-b7c3-4b7bd5abd22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Who are you?\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prompt = \"Who are you?\"\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = llm.predict(prompt)\n",
    "    print(f\"Response: {response}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf668bd-4539-4520-af96-59637f82a096",
   "metadata": {},
   "source": [
    "### Clean up cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b59ae0-355b-4e2e-947c-d6bc90cffc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'llm' in globals():\n",
    "    del llm\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92779f-95a0-4cf0-9561-664c65a0754f",
   "metadata": {},
   "source": [
    "## Llama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b66d542-5614-41a3-a668-916121350c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.config.settings:Successfully loaded configuration from /Users/jplfaria/repos/ModelSEEDagent/config/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from: /Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e799e902ecfc4fda9ce5a2529a8ff94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Who are you?\n",
      "Response: Of course! One example of a drug that affects the metabolism of a cell is the antibiotic doxycycline. Doxycycline is used to treat various infections caused by bacteria. It works by inhibiting the growth of bacteria. However, doxycycline can also affect the metabolism of the cell. It can cause changes in the metabolism of the cell, such as the production of toxic molecules.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config.settings import load_config\n",
    "from src.llm import LLMFactory\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Get complete path including snapshot\n",
    "model_path = \"/Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062\"\n",
    "\n",
    "# Create local LLM config with longer max_tokens\n",
    "local_llm_config = {\n",
    "    \"llm_name\": \"llama-3.2-3b\",\n",
    "    \"system_content\": config.local.system_content,\n",
    "    \"path\": model_path,\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"max_tokens\": 5000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"trust_remote_code\": True,  # Added this flag\n",
    "    \"use_fast\": False  # Added to use basic tokenizer\n",
    "}\n",
    "\n",
    "print(f\"\\nLoading model from: {model_path}\")\n",
    "# Create LLM and test\n",
    "llm = LLMFactory.create(\"local\", local_llm_config)\n",
    "\n",
    "try:\n",
    "    prompt = \"Who are you?\"\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = llm.predict(prompt)\n",
    "    print(f\"Response: {response}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up\n",
    "    if 'llm' in locals():\n",
    "        del llm\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae11762-b29a-47a3-8b36-ba6cb727eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.config.settings:Successfully loaded configuration from /Users/jplfaria/repos/ModelSEEDagent/config/config.yaml\n",
      "INFO:src.config.settings:Successfully loaded configuration from /Users/jplfaria/repos/ModelSEEDagent/config/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running Direct Analysis ===\n",
      "\n",
      "FBA Analysis:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cobra.core.model:The current solver interface glpk doesn't support setting the optimality tolerance.\n",
      "INFO:src.tools.cobra.utils:Successfully loaded model: iML1515\n",
      "INFO:cobra.core.model:The current solver interface glpk doesn't support setting the optimality tolerance.\n",
      "INFO:cobra.core.model:The current solver interface glpk doesn't support setting the optimality tolerance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growth Rate: 0.876998 h⁻¹\n",
      "\n",
      "Top 10 Most Active Reactions:\n",
      "    ATPS4rpp:  70.432518\n",
      "      H2Otex: -47.162367\n",
      "      H2Otpp: -47.162367\n",
      "    EX_h2o_e:  47.162367\n",
      "  CYTBO3_4pp:  44.256304\n",
      "    NADH16pp:  37.997055\n",
      "    EX_co2_e:  24.003301\n",
      "      CO2tpp: -24.003301\n",
      "      CO2tex: -24.003301\n",
      "       O2tpp:  22.131771\n",
      "\n",
      "Model Analysis:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.tools.cobra.utils:Successfully loaded model: iML1515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Statistics:\n",
      "num_reactions: 2712\n",
      "num_metabolites: 1877\n",
      "num_genes: 1516\n",
      "num_subsystems: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5e827a45754cf29dd53f1e6722d4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Agent Analysis ===\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Agent analysis failed: Local LLM Error: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.config.settings import load_config\n",
    "from src.llm import LLMFactory\n",
    "from src.tools import ToolRegistry\n",
    "from src.agents import AgentFactory\n",
    "\n",
    "# Set up logging and load configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "config = load_config()  # Make config global\n",
    "\n",
    "def run_direct_analysis(model_path: str, config_obj: dict) -> None:\n",
    "    \"\"\"Run direct analysis without agent\"\"\"\n",
    "    print(\"=== Running Direct Analysis ===\\n\")\n",
    "    \n",
    "    # Run FBA Analysis\n",
    "    print(\"FBA Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    fba_tool = ToolRegistry.create_tool(\"run_metabolic_fba\", {\n",
    "        \"name\": \"run_metabolic_fba\",\n",
    "        \"description\": \"Run FBA analysis\",\n",
    "        \"fba_config\": {\n",
    "            \"default_objective\": getattr(config_obj.tools.configs.fba_config, \"default_objective\", \"biomass_reaction\"),\n",
    "            \"solver\": getattr(config_obj.tools.configs.fba_config, \"solver\", \"glpk\"),\n",
    "            \"tolerance\": getattr(config_obj.tools.configs.fba_config, \"tolerance\", 1e-6),\n",
    "            \"additional_constraints\": {}\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    fba_result = fba_tool.run(str(model_path))\n",
    "    if fba_result.success:\n",
    "        print(f\"Growth Rate: {fba_result.data['objective_value']:.6f} h⁻¹\")\n",
    "        print(\"\\nTop 10 Most Active Reactions:\")\n",
    "        fluxes = dict(sorted(fba_result.data['significant_fluxes'].items(), \n",
    "                           key=lambda x: abs(x[1]), reverse=True)[:10])\n",
    "        for rxn, flux in fluxes.items():\n",
    "            print(f\"{rxn:>12}: {flux:>10.6f}\")\n",
    "    \n",
    "    # Run Model Analysis\n",
    "    print(\"\\nModel Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    analysis_tool = ToolRegistry.create_tool(\"analyze_metabolic_model\", {\n",
    "        \"name\": \"analyze_metabolic_model\",\n",
    "        \"description\": \"Analyze model structure\",\n",
    "        \"analysis_config\": {\n",
    "            \"flux_threshold\": 1e-6,\n",
    "            \"include_subsystems\": True,\n",
    "            \"track_metabolites\": True,\n",
    "            \"include_reactions\": None\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    analysis_result = analysis_tool.run(str(model_path))\n",
    "    if analysis_result.success:\n",
    "        print(\"\\nBasic Statistics:\")\n",
    "        stats = analysis_result.data['model_statistics']\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "def run_interactive_analysis():\n",
    "    \"\"\"Run interactive analysis with model loading\"\"\"\n",
    "    # Load configuration\n",
    "    config = load_config()\n",
    "    \n",
    "    # Define model path\n",
    "    model_path = Path.cwd().parent / \"data\" / \"models\" / \"iML1515.xml\"\n",
    "    \n",
    "    # First run direct analysis\n",
    "    run_direct_analysis(model_path, config)\n",
    "\n",
    "    # Local LLM Configuration\n",
    "    model_path = \"/Users/jplfaria/.llama/checkpoints/Llama3.2-3B-HuggingFace/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062\"\n",
    "    local_llm_config = {\n",
    "        \"llm_name\": \"llama-3.2-3b\",\n",
    "        \"system_content\": config.local.system_content,\n",
    "        \"path\": model_path,\n",
    "        \"device\": \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Create LLM\n",
    "    llm = LLMFactory.create(\"local\", local_llm_config)\n",
    "    \n",
    "    # Create tools for agent\n",
    "    tools = [\n",
    "        ToolRegistry.create_tool(\"run_metabolic_fba\", {\n",
    "            \"name\": \"run_metabolic_fba\",\n",
    "            \"description\": \"Calculate optimal growth rate and fluxes\",\n",
    "            \"fba_config\": {\n",
    "                \"default_objective\": getattr(config.tools.configs.fba_config, \"default_objective\", \"biomass_reaction\"),\n",
    "                \"solver\": getattr(config.tools.configs.fba_config, \"solver\", \"glpk\"),\n",
    "                \"tolerance\": getattr(config.tools.configs.fba_config, \"tolerance\", 1e-6),\n",
    "                \"additional_constraints\": {}\n",
    "            }\n",
    "        }),\n",
    "        ToolRegistry.create_tool(\"analyze_metabolic_model\", {\n",
    "            \"name\": \"analyze_metabolic_model\",\n",
    "            \"description\": \"Get model statistics and structure analysis\",\n",
    "            \"analysis_config\": {\n",
    "                \"flux_threshold\": 1e-6,\n",
    "                \"include_subsystems\": True,\n",
    "                \"track_metabolites\": True,\n",
    "                \"include_reactions\": None\n",
    "            }\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    # Create agent\n",
    "    agent = AgentFactory.create_agent(\n",
    "        agent_type=\"metabolic\",\n",
    "        llm=llm,\n",
    "        tools=tools,\n",
    "        config={\n",
    "            \"name\": \"metabolic_agent\",\n",
    "            \"description\": \"Analyze metabolic models\",\n",
    "            \"max_iterations\": 3,\n",
    "            \"verbose\": True,\n",
    "            \"handle_parsing_errors\": True\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Run agent analysis\n",
    "    print(\"\\n=== Running Agent Analysis ===\\n\")\n",
    "    result = agent.analyze_model(str(model_path))\n",
    "    \n",
    "    if result.success:\n",
    "        if 'final_answer' in result.data:\n",
    "            print(\"Agent's Analysis:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(result.data['final_answer'])\n",
    "    else:\n",
    "        print(f\"Agent analysis failed: {result.error}\")\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    run_interactive_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4b7ff-ad9d-4f9e-868d-2bda882b8831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelSEED Agent",
   "language": "python",
   "name": "modelseed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
