
================================================================================
FILE: config/config.yaml
================================================================================

llm:
  llm_backend: "argo"  # Change to "argo", "openai", or "local" to switch
  safety_settings:
    enabled: true
    max_api_calls: 100
    max_tokens: 128000

argo:
  user: "jplfaria"
  system_content: "You are a metabolic modeling expert."
  models:
    gpt35:
      api_base: "https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/"
      model_name: "gpt35"
    gpt4:
      api_base: "https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/"
      model_name: "gpt4"
    gpt4turbo:
      api_base: "https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/"
      model_name: "gpt4turbo"
    gpt4o:
      api_base: "https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/"
      model_name: "gpt4o"
    gpto1preview:
      api_base: "https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/"
      model_name: "gpto1preview"
    gpto1mini:
      api_base: "https://apps.inside.anl.gov/argoapi/api/v1/resource/chat/"
      model_name: "gpto1mini"
  default_model: "gpto1preview"
  streaming_api: "https://apps-dev.inside.anl.gov/argoapi/api/v1/resource/streamchat/"

local:
  model_name: "llama-3.2-3b"
  model_path: "/Users/jplfaria/.llama/checkpoints/Llama3.2-3B"
  system_content: "You are a metabolic modeling expert."
  device: "mps"
  max_tokens: 500
  temperature: 0.7
  models:
    llama-3.1-8b:
      model_name: "llama-3.1-8b"
      model_path: "/Users/jplfaria/.llama/checkpoints/Llama3.1-8B"
      device: "mps"
      max_tokens: 500
      temperature: 0.7
    llama-3.2-3b:
      model_name: "llama-3.2-3b"
      model_path: "/Users/jplfaria/.llama/checkpoints/Llama3.2-3B"
      device: "mps"
      max_tokens: 500
      temperature: 0.7

tools:
  configs:
    fba_config:
      default_objective: "biomass_reaction"
      solver: "glpk"
      tolerance: 1e-6
      simulation_method: "pfba"  # Options: "fba", "pfba", "geometric", "slim", etc.

agent:
  max_iterations: 5
  verbose: false
  handle_parsing_errors: true
  default_type: "metabolic"

################################################################################
END OF FILE: config/config.yaml
################################################################################

================================================================================
FILE: config/prompts/metabolic.yaml
================================================================================

# config/prompts/metabolic.yaml
description: "Prompts for metabolic modeling agent"
version: "1.1"

# Base system prompt
system_prompt: |
  You are an AI assistant specialized in metabolic modeling. Your expertise includes:
  - Flux Balance Analysis (FBA)
  - Metabolic network analysis
  - Pathway analysis
  - Model validation and improvement
  - Integration of genomic and metabolic data

# Agent prompts
agent:
  prefix: |
    You are a metabolic modeling expert. Analyze metabolic models using the available tools.
    IMPORTANT: Follow these rules exactly:
    1. Only provide ONE response type at a time - either an Action or a Final Answer, never both.
    2. Use "Action" when you need to call a tool.
    3. Use "Final Answer" only when you have all necessary information and are done.

    Previous Results:
    {tool_results}

    Available Tools:
    - run_metabolic_fba: Run FBA on a model to calculate growth rates and reaction fluxes.
    - analyze_metabolic_model: Analyze model structure and network properties.
    - check_missing_media: Check for missing media components that might be preventing growth.
    - find_minimal_media: Determine the minimal set of media components required for growth.
    - analyze_reaction_expression: Analyze reaction expression (fluxes) under provided media.
    - identify_auxotrophies: Identify auxotrophies by testing the effect of removing candidate nutrients.

  format_instructions: |
    Use this EXACT format - do not deviate from it:

    When using a tool:
      Thought: [your reasoning]
      Action: tool_name  # Choose one from: run_metabolic_fba, analyze_metabolic_model, check_missing_media, find_minimal_media, analyze_reaction_expression, identify_auxotrophies
      Action Input: [input for the tool, e.g. model path or a JSON with parameters]
      Observation: [result from the tool]
      ... (repeat as needed)

    When providing final answer:
      Thought: [summarize findings]
      Final Answer: [final summary]

    Important:
      - Use only the above tool names.
      - Do not combine an Action and a Final Answer in one response.
      - Keep responses concise and structured.
      - Do not add extra words to tool names.

  suffix: |
    Question: {input}
    {agent_scratchpad}

# Tool-specific prompts
tools:
  run_metabolic_fba:
    description: |
      Run Flux Balance Analysis (FBA) on a metabolic model and interpret the results.
      Input: Path to an SBML model file.
      Returns: Objective value, significant reaction fluxes, and related pathway activities.
    usage: |
      To use this tool:
      1. Provide the path to a valid SBML model file.
      2. Optionally specify an objective reaction.
      3. Review the fluxes and activity data returned.
    examples:
      - input: "path/to/model.xml"
        description: "Basic FBA analysis with default objective"

  analyze_metabolic_model:
    description: |
      Analyze structural properties of a metabolic model including reaction connectivity,
      pathway completeness, and potential gaps.
      Input: Path to a valid SBML model file.
    usage: |
      To use this tool:
      1. Provide the path to a valid SBML model file.
      2. Review the analysis results provided.
    examples:
      - input: "path/to/model.xml"
        description: "Complete model structure analysis"

  check_missing_media:
    description: |
      Check for missing media components that may be preventing growth.
      Input: Path to an SBML model file.
      The tool runs FBA on the current media, then tests supplementation of essential nutrients.
    usage: |
      To use this tool:
      1. Provide the path to a valid SBML model file.
      2. The tool will report if the model grows or if specific nutrients, when supplemented, restore growth.
    examples:
      - input: "path/to/model.xml"
        description: "Test for missing media components"

  find_minimal_media:
    description: |
      Determine the minimal media formulation required for model growth.
      Input: A JSON object with:
             - model_path: Path to an SBML model file.
             - media: A dictionary of complete media conditions.
    usage: |
      To use this tool:
      1. Provide the model path and a complete media formulation (e.g., {"EX_glc__D_e": (-10, 1000), ...}).
      2. The tool will iteratively remove non-essential nutrients.
      3. Review the minimal media formulation returned.
    examples:
      - input: '{"model_path": "path/to/model.xml", "media": {"EX_glc__D_e": (-10, 1000), "EX_nh4_e": (-10, 1000), ...}}'
        description: "Find minimal media for the model"

  analyze_reaction_expression:
    description: |
      Analyze reaction expression by running FBA under specified media conditions.
      Input: A JSON object with:
             - model_path: Path to an SBML model file.
             - media: A dictionary of media conditions.
      Returns: Reactions with significant flux values and the model growth rate.
    usage: |
      To use this tool:
      1. Provide the model path and media conditions.
      2. The tool will run FBA and return a list of reactions with fluxes above a threshold.
    examples:
      - input: '{"model_path": "path/to/model.xml", "media": {"EX_glc__D_e": (-10, 1000), ...}}'
        description: "Analyze reaction expression under specified media"

  identify_auxotrophies:
    description: |
      Identify potential auxotrophies by testing the effect of removing candidate external nutrients.
      Input: A JSON object with:
             - model_path: Path to an SBML model file.
             - media: A dictionary of current media conditions.
      Returns: List of candidate nutrients whose removal causes growth to drop below a threshold.
    usage: |
      To use this tool:
      1. Provide the model path and current media conditions.
      2. The tool will remove the uptake of each candidate nutrient one by one.
      3. If growth is compromised upon removal, that nutrient is flagged as an auxotrophy.
    examples:
      - input: '{"model_path": "path/to/model.xml", "media": {"EX_glc__D_e": (-10, 1000), ...}}'
        description: "Identify auxotrophies in the model"

# Response templates
responses:
  tool_result: |
    Tool Results:
    {tool_output}

    Status: {status}
    Success: {success}

    Next steps to consider:
    1. {next_step_1}
    2. {next_step_2}
    3. {next_step_3}

  error_response: |
    Error encountered: {error_message}

    Troubleshooting steps:
    1. Check model file path and format.
    2. Verify tool input parameters.
    3. Review tool permissions and access.

    Additional context: {error_context}

################################################################################
END OF FILE: config/prompts/metabolic.yaml
################################################################################

================================================================================
FILE: config/prompts/rast.yaml
================================================================================

# RAST Analysis Agent Prompts

description: "Prompts for RAST annotation agent"
version: "1.0"

# Base system prompt
system_prompt: |
  You are an AI assistant specialized in genome annotation and metabolic model construction.
  Your expertise includes:
  - RAST annotation analysis
  - Metabolic reconstruction
  - Gene-protein-reaction (GPR) associations
  - Pathway identification
  - Integration with metabolic models

# Agent prompts
agent:
  prefix: |
    You are an AI assistant specialized in genome annotation and metabolic reconstruction.
    You help analyze RAST annotations and integrate them with metabolic models.
    Available tools:

    {tools}

    When working with genome annotations, consider:
    1. Annotation quality and coverage
    2. Metabolic pathway completeness
    3. Gene-protein-reaction associations
    4. Integration with existing models
    5. Identification of missing or uncertain annotations

  format_instructions: |
    Use the following format:

    Question: The input question or task
    Thought: Analyze the question and determine your approach
    Action: Choose one of these tools [{tool_names}]
    Action Input: The input for the tool
    Observation: The result of the action
    ... (repeat Thought/Action/Action Input/Observation if needed)
    Thought: Reflect on what you've learned and what to do next
    Final Answer: The comprehensive answer to the original question

  suffix: |
    Begin!

    Question: {input}
    {agent_scratchpad}

# Tool-specific prompts
tools:
  annotation_analysis:
    description: |
      Analyze RAST genome annotations and identify metabolic functions.
      Input should be the path to the RAST annotation file.
      Returns detailed analysis of metabolic capabilities and pathway coverage.

    usage: |
      To use this tool:
      1. Provide the path to RAST annotation file
      2. Optionally specify pathways of interest
      3. Review the functional analysis results

    examples:
      - input: "path/to/rast_annotations.json"
        description: "Complete annotation analysis"
      - input: '{"annotation_path": "path/to/rast_annotations.json", "pathway": "Central Carbon Metabolism"}'
        description: "Pathway-specific analysis"

  model_integration:
    description: |
      Integrate RAST annotations with metabolic models.
      Maps annotations to model reactions and identifies potential new reactions.

    usage: |
      To use this tool:
      1. Provide paths to both RAST annotations and metabolic model
      2. Review mapping results and suggested additions
      3. Evaluate confidence scores for mappings

    examples:
      - input: '{"annotation_path": "annotations.json", "model_path": "model.xml"}'
        description: "Basic integration analysis"
      - input: '{"annotation_path": "annotations.json", "model_path": "model.xml", "confidence_threshold": 0.8}'
        description: "Integration with confidence filtering"

# Response templates
responses:
  annotation_report: |
    Annotation Analysis Results:

    Coverage:
    - Total Genes: {num_genes}
    - Annotated Functions: {num_functions}
    - Metabolic Genes: {num_metabolic}

    Pathway Coverage:
    {pathway_coverage}

    Key Findings:
    1. {finding_1}
    2. {finding_2}
    3. {finding_3}

    Recommendations:
    1. {recommendation_1}
    2. {recommendation_2}
    3. {recommendation_3}

  integration_report: |
    Model Integration Results:

    Mapping Statistics:
    - Mapped Reactions: {num_mapped}
    - New Reactions: {num_new}
    - Conflicting Mappings: {num_conflicts}

    Changes by Subsystem:
    {subsystem_changes}

    Suggested Actions:
    1. {action_1}
    2. {action_2}
    3. {action_3}

################################################################################
END OF FILE: config/prompts/rast.yaml
################################################################################

================================================================================
FILE: notebooks/utils.py
================================================================================

from pathlib import Path
from typing import List, Optional
from src.config.settings import load_config
from src.llm import LLMFactory
from src.tools import ToolRegistry
from src.agents import AgentFactory

def setup_metabolic_agent(
    model_name: str = "gpt4",
    tool_names: Optional[List[str]] = None
):
    """
    Set up a metabolic agent with common configurations.

    Args:
        model_name: Name of the LLM model to use
        tool_names: List of tool names to initialize

    Returns:
        tuple: (agent, config)
    """
    # Load configuration
    config = load_config()

    # Set up LLM
    config["argo"]["default_model"] = model_name
    llm = LLMFactory.create("argo", config["argo"])

    # Set up tools
    if tool_names is None:
        tool_names = ["run_metabolic_fba", "analyze_metabolic_model"]

    tools = [
        ToolRegistry.create_tool(name, config["tools"].get(name, {}))
        for name in tool_names
    ]

    # Create agent
    agent = AgentFactory.create_agent(
        agent_type="metabolic",
        llm=llm,
        tools=tools,
        config=config["agent"]
    )

    return agent, config

# Example usage in notebooks:
# from utils import setup_metabolic_agent
# agent, config = setup_metabolic_agent(model_name="gpt4")

################################################################################
END OF FILE: notebooks/utils.py
################################################################################

================================================================================
FILE: src/agents/__init__.py
================================================================================

from .base import BaseAgent, AgentResult
from .metabolic import MetabolicAgent
from .factory import AgentFactory

__all__ = [
    'BaseAgent',
    'AgentResult',
    'MetabolicAgent',
    'AgentFactory'
]

################################################################################
END OF FILE: src/agents/__init__.py
################################################################################

================================================================================
FILE: src/agents/base.py
================================================================================

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from langchain.agents import AgentExecutor
from langchain_core.language_models.base import BaseLanguageModel
from langchain_core.prompts import PromptTemplate
from ..llm.base import BaseLLM
from ..tools.base import BaseTool

class AgentResult(BaseModel):
    """Standardized result format for agent operations"""
    model_config = {"protected_namespaces": ()}
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = Field(default_factory=dict)
    intermediate_steps: List[Dict[str, Any]] = Field(default_factory=list)
    error: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class AgentConfig(BaseModel):
    """Base configuration for agents"""
    model_config = {"protected_namespaces": ()}
    name: str = "default"
    description: str = ""
    max_iterations: int = 5
    verbose: bool = False
    handle_parsing_errors: bool = True
    default_type: str = "metabolic"
    additional_config: Dict[str, Any] = Field(default_factory=dict)

class BaseAgent(ABC):
    """Abstract base class for all agents"""

    def __init__(
        self,
        llm: BaseLLM,
        tools: List[BaseTool],
        config: Dict[str, Any] | AgentConfig
    ):
        self.config = config if isinstance(config, AgentConfig) else AgentConfig(**dict(config))
        self.llm = llm
        self.tools = tools
        self._agent_executor = None
        self._setup_agent()

    @abstractmethod
    def _create_prompt(self) -> PromptTemplate:
        """Create the prompt template for the agent"""
        pass

    @abstractmethod
    def _create_agent(self) -> AgentExecutor:
        """Create the LangChain agent executor"""
        pass

    def _setup_agent(self) -> None:
        """Set up the agent executor"""
        self._agent_executor = self._create_agent()

    def _format_result(self, result: Dict[str, Any]) -> AgentResult:
        """Format the raw agent result into standardized format"""
        # Handle the case where result might be None
        if not result:
            return AgentResult(
                success=False,
                message="No result produced",
                error="Agent execution produced no result",
                data={},
                intermediate_steps=[]
            )

        # Extract intermediate steps and parse them
        steps = result.get("intermediate_steps", [])
        formatted_steps = []
        for step in steps:
            if isinstance(step, tuple) and len(step) == 2:
                action, observation = step
                formatted_step = {
                    "action": action.tool if hasattr(action, "tool") else str(action),
                    "action_input": action.tool_input if hasattr(action, "tool_input") else str(action),
                    "observation": str(observation)
                }
                formatted_steps.append(formatted_step)

        # Create the result
        return AgentResult(
            success="error" not in result,
            message=result.get("output", "No output provided"),
            data={
                "final_answer": result.get("output", ""),
                "tool_results": formatted_steps
            },
            intermediate_steps=formatted_steps,
            error=result.get("error"),
            metadata={
                "iterations": len(formatted_steps),
                "tools_used": self._get_tools_used(formatted_steps)
            }
        )

    def _get_tools_used(self, steps: List[Dict[str, Any]]) -> Dict[str, int]:
        """Track which tools were used and how many times"""
        tool_usage = {}
        for step in steps:
            tool_name = step.get("action", "unknown")
            if isinstance(tool_name, str):
                tool_usage[tool_name] = tool_usage.get(tool_name, 0) + 1
        return tool_usage

    def run(self, input_data: Dict[str, Any]) -> AgentResult:
        """Run the agent on the input data"""
        try:
            result = self._agent_executor.invoke(input_data)
            return self._format_result(result)
        except Exception as e:
            return AgentResult(
                success=False,
                message="Agent execution failed",
                error=str(e),
                data={}
            )

    async def arun(self, input_data: Dict[str, Any]) -> AgentResult:
        """Run the agent asynchronously"""
        try:
            result = await self._agent_executor.ainvoke(input_data)
            return self._format_result(result)
        except Exception as e:
            return AgentResult(
                success=False,
                message="Agent execution failed",
                error=str(e),
                data={}
            )

    def add_tool(self, tool: BaseTool) -> None:
        """Add a new tool to the agent"""
        self.tools.append(tool)
        self._setup_agent()

    def remove_tool(self, tool_name: str) -> None:
        """Remove a tool from the agent"""
        self.tools = [t for t in self.tools if t.name != tool_name]
        self._setup_agent()

################################################################################
END OF FILE: src/agents/base.py
################################################################################

================================================================================
FILE: src/agents/factory.py
================================================================================

from typing import Dict, Any, List, Type
from .base import BaseAgent
from .metabolic import MetabolicAgent
from ..llm.base import BaseLLM
from ..tools.base import BaseTool

class AgentFactory:
    """Factory class for creating agent instances"""

    _agent_registry: Dict[str, Type[BaseAgent]] = {
        "metabolic": MetabolicAgent,
        # Add more agent types here as they are implemented
    }

    @classmethod
    def register_agent(cls, name: str, agent_class: Type[BaseAgent]) -> None:
        """
        Register a new agent class.

        Args:
            name: Name identifier for the agent type
            agent_class: Agent class to register
        """
        if not issubclass(agent_class, BaseAgent):
            raise ValueError(f"Agent class must inherit from BaseAgent: {agent_class}")
        cls._agent_registry[name] = agent_class

    @classmethod
    def create_agent(
        cls,
        agent_type: str,
        llm: BaseLLM,
        tools: List[BaseTool],
        config: Dict[str, Any]
    ) -> BaseAgent:
        """
        Create an agent instance.

        Args:
            agent_type: Type of agent to create
            llm: Language model instance
            tools: List of tools for the agent
            config: Agent configuration

        Returns:
            Configured agent instance

        Raises:
            ValueError: If agent type is not registered
        """
        if agent_type not in cls._agent_registry:
            raise ValueError(
                f"Unknown agent type: {agent_type}. "
                f"Available types: {list(cls._agent_registry.keys())}"
            )

        agent_class = cls._agent_registry[agent_type]
        return agent_class(llm=llm, tools=tools, config=config)

    @classmethod
    def list_available_agents(cls) -> List[str]:
        """List all registered agent types"""
        return list(cls._agent_registry.keys())

# Example usage of the factory
def create_metabolic_agent(
    llm: BaseLLM,
    tools: List[BaseTool],
    config: Dict[str, Any]
) -> MetabolicAgent:
    """
    Convenience function to create a metabolic modeling agent.

    Args:
        llm: Language model instance
        tools: List of tools for the agent
        config: Agent configuration

    Returns:
        Configured MetabolicAgent instance
    """
    return AgentFactory.create_agent(
        agent_type="metabolic",
        llm=llm,
        tools=tools,
        config=config
    )

################################################################################
END OF FILE: src/agents/factory.py
################################################################################

================================================================================
FILE: src/agents/metabolic-Copy1.py
================================================================================

from typing import Dict, Any, List, Optional, Union
import json
import logging
import re
import os
from pathlib import Path
from datetime import datetime

from langchain.agents import AgentExecutor, create_react_agent
from langchain.agents.output_parsers import ReActSingleInputOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.agents import AgentAction, AgentFinish
from .base import BaseAgent, AgentResult, AgentConfig
from ..tools.base import BaseTool, ToolResult
from ..llm.base import BaseLLM
from ..config.prompts import load_prompts, load_prompt_template

# Additional imports for vector store functionality
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

logger = logging.getLogger(__name__)

# -------------------------------
# Simulation Results Store
# -------------------------------
class SimulationResultsStore:
    """
    A store for simulation results from COBRApy simulations (FBA, pFBA, geometric FBA, FVA, etc.)
    It saves results in a JSON format and exports fluxes as CSV files.
    """
    def __init__(self):
        self.results = {}

    def save_results(self, tool_name: str, model_id: str, solution, additional_metadata: Dict[str, Any] = None) -> str:
        timestamp = datetime.now().strftime("%Y-%m-%dT%H%M%S")
        result_id = f"{tool_name}_{model_id}_{timestamp}"
        # Assume solution.fluxes is a pandas Series; convert to dict
        result_data = {
            "objective_value": float(solution.objective_value),
            "fluxes": solution.fluxes.to_dict(),
            "status": solution.status,
            "timestamp": timestamp,
            "model_id": model_id,
            "tool": tool_name,
            "metadata": additional_metadata or {}
        }
        self.results[result_id] = result_data
        logger.info(f"SimulationResultsStore: Saved results for {tool_name} with ID {result_id}.")
        return result_id

    def export_results(self, result_id: str, output_dir: str) -> Dict[str, str]:
        """Exports the simulation result as a JSON file and fluxes as a CSV file."""
        if result_id not in self.results:
            logger.error(f"SimulationResultsStore: No results found for ID {result_id}.")
            return {}
        os.makedirs(output_dir, exist_ok=True)
        result = self.results[result_id]
        json_filename = f"{result_id}.json"
        json_path = os.path.join(output_dir, json_filename)
        with open(json_path, 'w') as f:
            json.dump(result, f, indent=2)
        # Export fluxes as CSV
        try:
            import pandas as pd
            flux_df = pd.DataFrame.from_dict(result["fluxes"], orient='index', columns=['flux'])
            csv_filename = f"{result_id}_fluxes.csv"
            csv_path = os.path.join(output_dir, csv_filename)
            flux_df.to_csv(csv_path)
        except Exception as e:
            logger.error(f"SimulationResultsStore: Error exporting fluxes CSV: {e}")
            csv_path = ""
        logger.info(f"SimulationResultsStore: Exported results to {json_path} and {csv_path}.")
        return {"json_file": json_path, "csv_file": csv_path}

# -------------------------------
# End SimulationResultsStore
# -------------------------------

# -------------------------------
# Simple Vector Store for Memory
# -------------------------------
class SimpleVectorStore:
    """
    A simple vector store using TF-IDF embeddings for retrieval.
    """
    def __init__(self):
        self.documents: List[str] = []
        self.vectorizer = TfidfVectorizer()
        self.embeddings = None

    def add_document(self, text: str) -> None:
        self.documents.append(text)
        self.embeddings = self.vectorizer.fit_transform(self.documents)
        logger.info(f"VectorStore: Added document (len={len(text)}). Total documents: {len(self.documents)}.")
        print(f"DEBUG: VectorStore added document; count = {len(self.documents)}.")

    def query(self, query_text: str, top_n: int = 1) -> List[str]:
        if not self.documents or self.embeddings is None:
            logger.info("VectorStore: No documents available for query.")
            print("DEBUG: VectorStore empty on query.")
            return []
        query_vec = self.vectorizer.transform([query_text])
        sim = cosine_similarity(query_vec, self.embeddings)
        top_indices = np.argsort(sim[0])[-top_n:][::-1]
        logger.info(f"VectorStore: Query '{query_text}' returned indices: {top_indices}.")
        print(f"DEBUG: VectorStore query indices: {top_indices}.")
        return [self.documents[i] for i in top_indices]
# -------------------------------
# End SimpleVectorStore
# -------------------------------

class CustomReActOutputParser(ReActSingleInputOutputParser):
    """Custom parser for combined Action/Final Answer responses."""
    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:
        if "For troubleshooting" in text:
            text = text.split("For troubleshooting")[0].strip()
        sequences = re.split(r'\nThought:', text)
        if not sequences:
            return AgentFinish({"output": "No valid response generated"}, "")
        last_sequence = sequences[-1].strip()
        if "Final Answer:" in last_sequence:
            final_answer = last_sequence.split("Final Answer:")[-1].strip()
            return AgentFinish({"output": final_answer}, text)
        if "Action:" in last_sequence and "Action Input:" in last_sequence:
            action_match = re.search(r'Action: (.*?)(?:\n|$)', last_sequence)
            input_match = re.search(r'Action Input: (.*?)(?:\n|$)', last_sequence)
            if action_match and input_match:
                action = action_match.group(1).strip()
                action_input = input_match.group(1).strip()
                return AgentAction(action, action_input, text)
        return AgentFinish({"output": last_sequence}, text)

class MetabolicAgent(BaseAgent):
    """Agent for metabolic model analysis with enhanced memory, token management, simulation result export, and logging."""

    def __init__(self, llm: BaseLLM, tools: List[BaseTool], config: Dict[str, Any] | AgentConfig):
        self._current_tool_results = "No previous results"
        self._tools_dict = {t.tool_name: t for t in tools}
        self._tool_output_counters = {}
        super().__init__(llm, tools, config)
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_base = Path(__file__).parent.parent.parent / "logs"
        self.run_dir = self.log_base / f"run_{self.run_id}"
        self.run_dir.mkdir(parents=True, exist_ok=True)
        self.tool_dirs = {}
        for tool in tools:
            tool_dir = self.run_dir / tool.tool_name
            tool_dir.mkdir(parents=True, exist_ok=True)
            self.tool_dirs[tool.tool_name] = tool_dir
        self.exec_log = self.run_dir / "execution.json"
        with open(self.exec_log, 'w') as f:
            json.dump([], f)
        self.current_iteration = 0
        self._last_query = ""
        self.vector_store = SimpleVectorStore()
        self.simulation_store = SimulationResultsStore()  # New simulation results store
        logger.info(f"Created run directory: {self.run_dir}")
        print(f"DEBUG: Run directory created at {self.run_dir}")

    def _normalize_tool_name(self, tool_name: str) -> str:
        return tool_name.lower().replace(" ", "_")

    def _create_prompt(self) -> PromptTemplate:
        try:
            template = load_prompt_template("metabolic")
            if template:
                logger.info("Using prompt template from config")
                if "tool_names" not in template.input_variables:
                    template.input_variables.append("tool_names")
                if "tools" not in template.input_variables:
                    template.input_variables.append("tools")
                return template
        except Exception as e:
            logger.warning(f"Could not load prompt template from config: {e}. Using default.")
        return PromptTemplate(
            template=(
                "You are a metabolic modeling expert. Analyze metabolic models using the available tools.\n"
                "IMPORTANT: Follow these rules exactly:\n"
                "1. Only provide ONE response type at a time - either an Action or a Final Answer, never both\n"
                "2. Use Action when you need to call a tool\n"
                "3. Use Final Answer only when you have all necessary information and are done\n\n"
                "Previous Results:\n"
                "{tool_results}\n\n"
                "Available Tools:\n"
                "- run_metabolic_fba: Run FBA on a model to calculate growth rates and reaction fluxes.\n"
                "- analyze_metabolic_model: Analyze model structure and network properties.\n"
                "- check_missing_media: Check for missing media components that might be preventing growth.\n"
                "- find_minimal_media: Determine the minimal set of media components required for growth.\n"
                "- analyze_reaction_expression: Analyze reaction fluxes under provided media.\n"
                "- identify_auxotrophies: Identify auxotrophies by testing nutrient removal.\n\n"
                "Use this EXACT format - do not deviate from it:\n\n"
                "When using a tool:\n"
                "  Thought: [your reasoning]\n"
                "  Action: tool_name  # Choose one from the list above\n"
                "  Action Input: [input for the tool]\n"
                "  Observation: [result from the tool]\n"
                "... (repeat as needed)\n\n"
                "When providing final answer:\n"
                "  Thought: [summarize findings]\n"
                "  Final Answer: [final summary]\n\n"
                "Question: {input}\n"
                "{agent_scratchpad}"
            ),
            input_variables=["input", "agent_scratchpad", "tool_results", "tools", "tool_names"]
        )

    def _summarize_tool_results(self, results: str) -> str:
        summary_prompt = f"Please summarize the following simulation output concisely:\n\n{results}\n\nSummary:"
        summary = self.llm.predict(summary_prompt, max_tokens=150)
        print("DEBUG: Summarization prompt sent, summary obtained.")
        return summary.strip()

    def _log_tool_output(self, tool_name: str, content: Dict[str, Any]) -> None:
        print(f"DEBUG: Logging output for {tool_name}: {content}")
        try:
            tool_dir = self.tool_dirs.get(tool_name)
            if not tool_dir:
                logger.error(f"Log directory for {tool_name} not found.")
                print(f"DEBUG: Log directory for {tool_name} not found.")
                return
            counter = self._tool_output_counters.get(tool_name, 0) + 1
            self._tool_output_counters[tool_name] = counter
            timestamp = datetime.now().strftime("%H%M%S_%f")
            filename = f"{tool_name}_output_{counter}_{timestamp}.json"
            log_file = tool_dir / filename
            with open(log_file, 'w') as f:
                json.dump(content, f, indent=2)
            logger.debug(f"Logged {tool_name} output to {log_file}")
            print(f"DEBUG: Logged output for {tool_name} to {log_file}")
        except Exception as e:
            logger.error(f"Failed to log {tool_name} output: {e}")
            print(f"DEBUG: Failed to log output for {tool_name}: {e}")

    def _log_execution(self, step: str, content: Any) -> None:
        try:
            with open(self.exec_log, 'r') as f:
                log = json.load(f)
        except Exception:
            log = []
        large_content_threshold = self.config.get("summarization_threshold", 1000) if isinstance(self.config, dict) else 1000
        log_content = content
        if isinstance(content, str) and len(content) > large_content_threshold:
            file_name = f"large_output_{datetime.now().strftime('%H%M%S_%f')}.txt"
            output_file = self.run_dir / file_name
            try:
                with open(output_file, 'w') as f_out:
                    f_out.write(content)
                log_content = f"Large output stored in file: {str(output_file)}"
            except Exception as e:
                logger.error(f"Failed to write large output to file {output_file}: {e}")
                log_content = f"Failed to store large output; length: {len(content)}"
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "iteration": self.current_iteration,
            "token_usage": getattr(self.llm, '_tokens', None),
            "content": log_content
        }
        log.append(log_entry)
        try:
            with open(self.exec_log, 'w') as f:
                json.dump(log, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to update log file {self.exec_log}: {e}")

    def _format_tool_results(self, results: Dict[str, Any]) -> str:
        output = []
        for tool_name, data in results.items():
            if not isinstance(data, dict):
                try:
                    data = data.dict()
                except Exception:
                    data = {}
            if tool_name == "analyze_metabolic_model":
                output.append(f"Model contains {data.get('num_reactions', 'N/A')} reactions, {data.get('num_metabolites', 'N/A')} metabolites.")
            elif tool_name == "run_metabolic_fba":
                growth_rate = data.get("objective_value", "Unknown")
                # If simulation results include a result file, include its name.
                result_file = data.get("result_file", "N/A")
                output.append(f"FBA Analysis: Growth rate is {growth_rate}. (Results file: {result_file})")
        return "\n".join(output) if output else "No tool results available."

    def _get_tools_used(self, steps: List[Dict[str, Any]]) -> Dict[str, int]:
        tool_usage = {}
        for step in steps:
            tool_name = self._normalize_tool_name(step.get("action", "unknown"))
            if tool_name in self._tools_dict:
                tool_usage[tool_name] = tool_usage.get(tool_name, 0) + 1
        return tool_usage

    def _format_result(self, result: Dict[str, Any]) -> AgentResult:
        try:
            steps = result.get("intermediate_steps", [])
            output = result.get("output", "")
            formatted_steps = []
            tool_outputs = {}
            for step in steps:
                if isinstance(step, tuple) and len(step) == 2:
                    action, observation = step
                    tool_name = self._normalize_tool_name(
                        action.tool if hasattr(action, "tool") else str(action)
                    )
                    if tool_name in ["run_metabolic_fba", "analyze_metabolic_model", "check_missing_media", "find_minimal_media", "analyze_reaction_expression", "identify_auxotrophies"]:
                        tool_outputs[tool_name] = observation
                        step_data = {
                            "action": tool_name,
                            "action_input": getattr(action, "tool_input", str(action)),
                            "observation": observation
                        }
                        formatted_steps.append(step_data)
                        self._log_tool_output(tool_name, step_data)
            tool_results_str = self._format_tool_results(tool_outputs)
            summarization_threshold = self.config.get("summarization_threshold", 500) if isinstance(self.config, dict) else 500
            if self.llm.estimate_tokens(tool_results_str) > summarization_threshold:
                summarized = self._summarize_tool_results(tool_results_str)
                self._current_tool_results = summarized
            else:
                self._current_tool_results = tool_results_str
            # Add tool results to vector store for memory (store as string)
            document_to_store = str(tool_results_str)
            if self.vector_store:
                self.vector_store.add_document(document_to_store)
                retrieved_context = self.vector_store.query(str(self._last_query), top_n=1)
            else:
                retrieved_context = []
            if isinstance(output, str):
                output = output.split("For troubleshooting")[0].strip()
                output = re.sub(r'Thought:|Action:|Action Input:|Observation:', '', output).strip()
            try:
                with open(self.exec_log, 'r') as f:
                    log_entries = json.load(f)
                last_log = log_entries[-1] if log_entries else {}
            except Exception:
                last_log = {}
            return AgentResult(
                success=True,
                message="Analysis completed successfully",
                data={
                    "final_answer": output or "Analysis completed",
                    "tool_results": formatted_steps,
                    "summary": self._current_tool_results,
                    "retrieved_context": retrieved_context,
                    "log_summary": f"Enhanced logging enabled. Run directory: {str(self.run_dir)}. Last log entry: {last_log}"
                },
                intermediate_steps=formatted_steps,
                metadata={
                    "iterations": len(formatted_steps),
                    "tools_used": self._get_tools_used(formatted_steps),
                    "last_tool": formatted_steps[-1]["action"] if formatted_steps else None
                }
            )
        except Exception as e:
            logger.exception("Error formatting agent result")
            return AgentResult(success=False, message="Error formatting agent result", error=str(e))

    def _create_agent(self) -> AgentExecutor:
        prompt = self._create_prompt()
        tools_renderer = lambda tools_list: "\n".join([f"- {tool.name}: {tool.description}" for tool in tools_list])
        prompt = prompt.partial(tools=tools_renderer(self.tools), tool_names=", ".join([t.name for t in self.tools]))
        from langchain.agents.react.agent import create_react_agent
        runnable = create_react_agent(llm=self.llm, tools=self.tools, prompt=prompt, output_parser=CustomReActOutputParser())

        # Create a dictionary that maps tool names to their instances
        tool_map = {tool.name: tool for tool in self.tools}

        # Create wrapped tools that inject output directory
        wrapped_tools = []
        for tool in self.tools:
            # Create a wrapped version of the tool's run method
            original_run = tool.run

            def wrapped_run(input_data, _tool=tool, _run_dir=self.run_dir):
                # If input is a string, convert to dict with model_path and output_dir
                if isinstance(input_data, str):
                    model_path = input_data
                    tool_dir = _run_dir / _tool.name
                    input_data = {
                        "model_path": model_path,
                        "output_dir": str(tool_dir)
                    }
                # If input is a dict but missing output_dir, add it
                elif isinstance(input_data, dict) and "model_path" in input_data and "output_dir" not in input_data:
                    tool_dir = _run_dir / _tool.name
                    input_data["output_dir"] = str(tool_dir)

                # Call the original run method with the enhanced input
                return original_run(input_data)

            # Replace the tool's run method with our wrapped version
            tool.run = wrapped_run.__get__(tool, type(tool))
            wrapped_tools.append(tool)

        # Create and return the agent executor with wrapped tools
        return AgentExecutor(
            agent=runnable,
            tools=wrapped_tools,
            verbose=self.config.verbose,
            max_iterations=self.config.max_iterations,
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )

    def analyze_model(self, query: str) -> AgentResult:
        try:
            logger.info(f"Starting model analysis with query: {query}")
            return self.run({"input": query, "tool_results": self._current_tool_results, "tools": "\n".join(f"- {t.name}: {t.description}" for t in self.tools)})
        except Exception as e:
            error_msg = f"Error in analyze_model: {str(e)}"
            logger.exception(error_msg)
            return AgentResult(success=False, message=error_msg, error=str(e))

    async def arun(self, input_data: Dict[str, Any]) -> AgentResult:
        try:
            if "tool_results" not in input_data:
                input_data.update({"tool_results": self._current_tool_results, "tools": "\n".join(f"- {t.name}: {t.description}" for t in self.tools)})
            logger.info(f"Starting async analysis with input: {input_data}")
            result = await self._agent_executor.ainvoke(input_data)
            formatted_result = self._format_result(result)
            self._log_execution("complete", formatted_result.dict())
            return formatted_result
        except Exception as e:
            error_msg = f"Async agent execution failed: {str(e)}"
            self._log_execution("error", {"error": error_msg})
            logger.error(error_msg)
            return AgentResult(success=False, message=error_msg, error=str(e))

################################################################################
END OF FILE: src/agents/metabolic-Copy1.py
################################################################################

================================================================================
FILE: src/agents/metabolic.py
================================================================================

from typing import Dict, Any, List, Optional, Union
import json
import logging
import re
import os
from pathlib import Path
from datetime import datetime

from langchain.agents import AgentExecutor, create_react_agent
from langchain.agents.output_parsers import ReActSingleInputOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.agents import AgentAction, AgentFinish
from .base import BaseAgent, AgentResult, AgentConfig
from ..tools.base import BaseTool, ToolResult
from ..llm.base import BaseLLM
from ..config.prompts import load_prompts, load_prompt_template

# Additional imports for vector store functionality
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

logger = logging.getLogger(__name__)

# -------------------------------
# Simulation Results Store
# -------------------------------
class SimulationResultsStore:
    """
    A store for simulation results from COBRApy simulations (FBA, pFBA, geometric FBA, FVA, etc.)
    It saves results in a JSON format and exports fluxes as CSV files.
    """
    def __init__(self):
        self.results = {}

    def save_results(self, tool_name: str, model_id: str, solution, additional_metadata: Dict[str, Any] = None) -> str:
        timestamp = datetime.now().strftime("%Y-%m-%dT%H%M%S")
        result_id = f"{tool_name}_{model_id}_{timestamp}"
        # Assume solution.fluxes is a pandas Series; convert to dict
        result_data = {
            "objective_value": float(solution.objective_value),
            "fluxes": solution.fluxes.to_dict(),
            "status": solution.status,
            "timestamp": timestamp,
            "model_id": model_id,
            "tool": tool_name,
            "metadata": additional_metadata or {}
        }
        self.results[result_id] = result_data
        logger.info(f"SimulationResultsStore: Saved results for {tool_name} with ID {result_id}.")
        return result_id

    def export_results(self, result_id: str, output_dir: str) -> Dict[str, str]:
        """Exports the simulation result as a JSON file and fluxes as a CSV file."""
        if result_id not in self.results:
            logger.error(f"SimulationResultsStore: No results found for ID {result_id}.")
            return {}
        os.makedirs(output_dir, exist_ok=True)
        result = self.results[result_id]
        json_filename = f"{result_id}.json"
        json_path = os.path.join(output_dir, json_filename)
        with open(json_path, 'w') as f:
            json.dump(result, f, indent=2)
        # Export fluxes as CSV
        try:
            import pandas as pd
            flux_df = pd.DataFrame.from_dict(result["fluxes"], orient='index', columns=['flux'])
            csv_filename = f"{result_id}_fluxes.csv"
            csv_path = os.path.join(output_dir, csv_filename)
            flux_df.to_csv(csv_path)
        except Exception as e:
            logger.error(f"SimulationResultsStore: Error exporting fluxes CSV: {e}")
            csv_path = ""
        logger.info(f"SimulationResultsStore: Exported results to {json_path} and {csv_path}.")
        return {"json_file": json_path, "csv_file": csv_path}

# -------------------------------
# End SimulationResultsStore
# -------------------------------

# -------------------------------
# Simple Vector Store for Memory
# -------------------------------
class SimpleVectorStore:
    """
    A simple vector store using TF-IDF embeddings for retrieval.
    """
    def __init__(self):
        self.documents: List[str] = []
        self.vectorizer = TfidfVectorizer()
        self.embeddings = None

    def add_document(self, text: str) -> None:
        self.documents.append(text)
        self.embeddings = self.vectorizer.fit_transform(self.documents)
        logger.info(f"VectorStore: Added document (len={len(text)}). Total documents: {len(self.documents)}.")

    def query(self, query_text: str, top_n: int = 1) -> List[str]:
        if not self.documents or self.embeddings is None:
            logger.info("VectorStore: No documents available for query.")
            return []
        query_vec = self.vectorizer.transform([query_text])
        sim = cosine_similarity(query_vec, self.embeddings)
        top_indices = np.argsort(sim[0])[-top_n:][::-1]
        logger.info(f"VectorStore: Query returned {len(top_indices)} results.")
        return [self.documents[i] for i in top_indices]
# -------------------------------
# End SimpleVectorStore
# -------------------------------

class CustomReActOutputParser(ReActSingleInputOutputParser):
    """Custom parser for combined Action/Final Answer responses."""
    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:
        if "For troubleshooting" in text:
            text = text.split("For troubleshooting")[0].strip()
        sequences = re.split(r'\nThought:', text)
        if not sequences:
            return AgentFinish({"output": "No valid response generated"}, "")
        last_sequence = sequences[-1].strip()
        if "Final Answer:" in last_sequence:
            final_answer = last_sequence.split("Final Answer:")[-1].strip()
            return AgentFinish({"output": final_answer}, text)
        if "Action:" in last_sequence and "Action Input:" in last_sequence:
            action_match = re.search(r'Action: (.*?)(?:\n|$)', last_sequence)
            input_match = re.search(r'Action Input: (.*?)(?:\n|$)', last_sequence)
            if action_match and input_match:
                action = action_match.group(1).strip()
                action_input = input_match.group(1).strip()
                return AgentAction(action, action_input, text)
        return AgentFinish({"output": last_sequence}, text)

class MetabolicAgent(BaseAgent):
    """Agent for metabolic model analysis with enhanced memory, token management, simulation result export, and logging."""

    def __init__(self, llm: BaseLLM, tools: List[BaseTool], config: Dict[str, Any] | AgentConfig):
        # Initialize variables needed before super().__init__
        self._current_tool_results = "No previous results"
        self._tools_dict = {t.tool_name: t for t in tools}
        self._tool_output_counters = {}

        # Set up logging directory structure BEFORE super().__init__
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_base = Path(__file__).parent.parent.parent / "logs"
        self.run_dir = self.log_base / f"run_{self.run_id}"
        self.run_dir.mkdir(parents=True, exist_ok=True)

        # Create directories for the new structure
        self.results_dir = self.run_dir / "results"
        self.results_dir.mkdir(exist_ok=True)
        self.observations_dir = self.run_dir / "observations"
        self.observations_dir.mkdir(exist_ok=True)
        self.steps_dir = self.run_dir / "steps"
        self.steps_dir.mkdir(exist_ok=True)

        # Call super().__init__ which will trigger _setup_agent()
        super().__init__(llm, tools, config)

        # Create the execution log
        self.exec_log = self.run_dir / "execution.json"
        with open(self.exec_log, 'w') as f:
            json.dump([], f)

        self.current_iteration = 0
        self._last_query = ""
        self.vector_store = SimpleVectorStore()
        self.simulation_store = SimulationResultsStore()

        logger.info(f"Created run directory with improved structure: {self.run_dir}")
        print(f"DEBUG: Run directory created at {self.run_dir}")

    def _normalize_tool_name(self, tool_name: str) -> str:
        return tool_name.lower().replace(" ", "_")

    def _create_prompt(self) -> PromptTemplate:
        try:
            template = load_prompt_template("metabolic")
            if template:
                logger.info("Using prompt template from config")
                if "tool_names" not in template.input_variables:
                    template.input_variables.append("tool_names")
                if "tools" not in template.input_variables:
                    template.input_variables.append("tools")
                return template
        except Exception as e:
            logger.warning(f"Could not load prompt template from config: {e}. Using default.")
        return PromptTemplate(
            template=(
                "You are a metabolic modeling expert. Analyze metabolic models using the available tools.\n"
                "IMPORTANT: Follow these rules exactly:\n"
                "1. Only provide ONE response type at a time - either an Action or a Final Answer, never both\n"
                "2. Use Action when you need to call a tool\n"
                "3. Use Final Answer only when you have all necessary information and are done\n\n"
                "Previous Results:\n"
                "{tool_results}\n\n"
                "Available Tools:\n"
                "- run_metabolic_fba: Run FBA on a model to calculate growth rates and reaction fluxes.\n"
                "- analyze_metabolic_model: Analyze model structure and network properties.\n"
                "- check_missing_media: Check for missing media components that might be preventing growth.\n"
                "- find_minimal_media: Determine the minimal set of media components required for growth.\n"
                "- analyze_reaction_expression: Analyze reaction fluxes under provided media.\n"
                "- identify_auxotrophies: Identify auxotrophies by testing nutrient removal.\n\n"
                "Use this EXACT format - do not deviate from it:\n\n"
                "When using a tool:\n"
                "  Thought: [your reasoning]\n"
                "  Action: tool_name  # Choose one from the list above\n"
                "  Action Input: [input for the tool]\n"
                "  Observation: [result from the tool]\n"
                "... (repeat as needed)\n\n"
                "When providing final answer:\n"
                "  Thought: [summarize findings]\n"
                "  Final Answer: [final summary]\n\n"
                "Question: {input}\n"
                "{agent_scratchpad}"
            ),
            input_variables=["input", "agent_scratchpad", "tool_results", "tools", "tool_names"]
        )

    def _summarize_tool_results(self, results: str) -> str:
        summary_prompt = f"Please summarize the following simulation output concisely:\n\n{results}\n\nSummary:"
        summary = self.llm.predict(summary_prompt, max_tokens=150)
        logger.info("Tool results summarization completed")
        return summary.strip()

    def _log_execution(self, step: str, content: Any) -> None:
        try:
            with open(self.exec_log, 'r') as f:
                log = json.load(f)
        except Exception:
            log = []
        large_content_threshold = self.config.get("summarization_threshold", 1000) if isinstance(self.config, dict) else 1000
        log_content = content
        if isinstance(content, str) and len(content) > large_content_threshold:
            file_name = f"large_output_{datetime.now().strftime('%H%M%S_%f')}.txt"
            output_file = self.run_dir / file_name
            try:
                with open(output_file, 'w') as f_out:
                    f_out.write(content)
                log_content = f"Large output stored in file: {str(output_file)}"
            except Exception as e:
                logger.error(f"Failed to write large output to file {output_file}: {e}")
                log_content = f"Failed to store large output; length: {len(content)}"
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "iteration": self.current_iteration,
            "token_usage": getattr(self.llm, '_tokens', None),
            "content": log_content
        }
        log.append(log_entry)
        try:
            with open(self.exec_log, 'w') as f:
                json.dump(log, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to update log file {self.exec_log}: {e}")

    def _format_tool_results(self, results: Dict[str, Any]) -> str:
        output = []
        for tool_name, data in results.items():
            if not isinstance(data, dict):
                try:
                    data = data.dict()
                except Exception:
                    data = {}
            if tool_name == "analyze_metabolic_model":
                output.append(f"Model contains {data.get('num_reactions', 'N/A')} reactions, {data.get('num_metabolites', 'N/A')} metabolites.")
            elif tool_name == "run_metabolic_fba":
                growth_rate = data.get("objective_value", "Unknown")
                # If simulation results include a result file, include its name.
                result_file = data.get("result_file", "N/A")
                output.append(f"FBA Analysis: Growth rate is {growth_rate}. (Results file: {result_file})")
        return "\n".join(output) if output else "No tool results available."

    def _get_tools_used(self, steps: List[Dict[str, Any]]) -> Dict[str, int]:
        tool_usage = {}
        for step in steps:
            tool_name = self._normalize_tool_name(step.get("action", "unknown"))
            if tool_name in self._tools_dict:
                tool_usage[tool_name] = tool_usage.get(tool_name, 0) + 1
        return tool_usage

    def _create_agent(self) -> AgentExecutor:
        prompt = self._create_prompt()
        tools_renderer = lambda tools_list: "\n".join([f"- {tool.name}: {tool.description}" for tool in tools_list])
        prompt = prompt.partial(tools=tools_renderer(self.tools), tool_names=", ".join([t.name for t in self.tools]))
        from langchain.agents.react.agent import create_react_agent
        runnable = create_react_agent(llm=self.llm, tools=self.tools, prompt=prompt, output_parser=CustomReActOutputParser())

        # Create the standard agent executor (without modifying tools)
        agent_executor = AgentExecutor(
            agent=runnable,
            tools=self.tools,
            verbose=self.config.verbose,
            max_iterations=self.config.max_iterations,
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )

        return agent_executor

    def _format_result(self, result: Dict[str, Any]) -> AgentResult:
        try:
            steps = result.get("intermediate_steps", [])
            output = result.get("output", "")
            formatted_steps = []
            tool_outputs = {}
            all_files = {}

            # Process each step and save files
            for step_idx, step in enumerate(steps):
                if isinstance(step, tuple) and len(step) == 2:
                    action, observation = step
                    tool_name = self._normalize_tool_name(
                        action.tool if hasattr(action, "tool") else str(action)
                    )

                    # Get action input
                    action_input = getattr(action, "tool_input", str(action))

                    # Process valid tool steps
                    if tool_name in ["run_metabolic_fba", "analyze_metabolic_model", "check_missing_media",
                                    "find_minimal_media", "analyze_reaction_expression", "identify_auxotrophies"]:

                        # Save observation to file
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                        # Save observation
                        observation_file = self.observations_dir / f"{timestamp}_{tool_name}_observation.json"
                        observation_data = {
                            "tool": tool_name,
                            "input": str(action_input),
                            "observation": str(observation),
                            "timestamp": timestamp,
                            "step": step_idx
                        }

                        try:
                            with open(observation_file, 'w') as f:
                                json.dump(observation_data, f, indent=2)
                            all_files[f"observation_{tool_name}_{step_idx}"] = str(observation_file)
                        except Exception as e:
                            logger.error(f"Error saving observation: {e}")

                        # Save tool results if available
                        if hasattr(observation, 'data') and observation.data:
                            try:
                                # Save results JSON
                                result_file = self.results_dir / f"{timestamp}_{tool_name}_result.json"
                                with open(result_file, 'w') as f:
                                    json.dump(observation.data, f, indent=2)
                                all_files[f"result_{tool_name}_{step_idx}"] = str(result_file)

                                # For FBA tools with fluxes, save CSV
                                if tool_name == "run_metabolic_fba" and "significant_fluxes" in observation.data:
                                    import pandas as pd
                                    flux_df = pd.DataFrame.from_dict(
                                        observation.data.get("significant_fluxes", {}),
                                        orient='index',
                                        columns=['flux']
                                    )
                                    flux_file = self.results_dir / f"{timestamp}_{tool_name}_fluxes.csv"
                                    flux_df.to_csv(flux_file)
                                    all_files[f"fluxes_{tool_name}_{step_idx}"] = str(flux_file)
                            except Exception as e:
                                logger.error(f"Error saving result files: {e}")

                        # Save step info
                        step_file = self.steps_dir / f"{timestamp}_{tool_name}_step.json"
                        step_data = {
                            "step": step_idx,
                            "tool": tool_name,
                            "input": str(action_input),
                            "timestamp": timestamp,
                            "files": all_files
                        }
                        try:
                            with open(step_file, 'w') as f:
                                json.dump(step_data, f, indent=2)
                            all_files[f"step_{tool_name}_{step_idx}"] = str(step_file)
                        except Exception as e:
                            logger.error(f"Error saving step data: {e}")

                        # Add to tool outputs and formatted steps
                        tool_outputs[tool_name] = observation
                        formatted_steps.append({
                            "action": tool_name,
                            "action_input": action_input,
                            "observation": observation,
                            "files": {k: v for k, v in all_files.items() if f"_{tool_name}_" in k}
                        })

            # Process tool results for summary
            tool_results_str = self._format_tool_results(tool_outputs)
            summarization_threshold = self.config.get("summarization_threshold", 500) if isinstance(self.config, dict) else 500

            if self.llm.estimate_tokens(tool_results_str) > summarization_threshold:
                summarized = self._summarize_tool_results(tool_results_str)
                self._current_tool_results = summarized
            else:
                self._current_tool_results = tool_results_str

            # Add to vector store for context
            document_to_store = str(tool_results_str)
            if self.vector_store:
                self.vector_store.add_document(document_to_store)
                retrieved_context = self.vector_store.query(str(self._last_query), top_n=1)
            else:
                retrieved_context = []

            # Clean output
            if isinstance(output, str):
                output = output.split("For troubleshooting")[0].strip()
                output = re.sub(r'Thought:|Action:|Action Input:|Observation:', '', output).strip()

            # Get latest log entry
            try:
                with open(self.exec_log, 'r') as f:
                    log_entries = json.load(f)
                last_log = log_entries[-1] if log_entries else {}
            except Exception:
                last_log = {}

            # Save final report
            if output:
                final_report_file = self.run_dir / f"final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                try:
                    with open(final_report_file, 'w') as f:
                        f.write(output)
                    all_files["final_report"] = str(final_report_file)
                except Exception as e:
                    logger.error(f"Failed to save final report: {e}")

            return AgentResult(
                success=True,
                message="Analysis completed successfully",
                data={
                    "final_answer": output or "Analysis completed",
                    "tool_results": formatted_steps,
                    "summary": self._current_tool_results,
                    "retrieved_context": retrieved_context,
                    "log_summary": f"Enhanced logging enabled. Run directory: {str(self.run_dir)}. Last log entry: {last_log}",
                    "files": all_files
                },
                intermediate_steps=formatted_steps,
                metadata={
                    "iterations": len(formatted_steps),
                    "tools_used": self._get_tools_used(formatted_steps),
                    "last_tool": formatted_steps[-1]["action"] if formatted_steps else None,
                    "run_dir": str(self.run_dir)
                }
            )
        except Exception as e:
            logger.exception("Error formatting agent result")
            return AgentResult(success=False, message="Error formatting agent result", error=str(e))

    def run(self, input_data: Dict[str, Any]) -> AgentResult:
        try:
            query = str(input_data.get("input", ""))
            self._last_query = query
            input_data.update({"input": query, "tool_results": self._current_tool_results})
            self._log_execution("start", {"query": query})
            result = self._agent_executor.invoke(input_data)
            formatted_result = self._format_result(result)
            self._log_execution("complete", formatted_result.dict())
            return formatted_result
        except Exception as e:
            self._log_execution("error", {"error": str(e)})
            logger.error(f"Agent execution failed: {e}")
            return AgentResult(success=False, message="Execution failed", error=str(e))

    def analyze_model(self, query: str) -> AgentResult:
        try:
            logger.info(f"Starting model analysis with query: {query}")
            return self.run({"input": query, "tool_results": self._current_tool_results, "tools": "\n".join(f"- {t.name}: {t.description}" for t in self.tools)})
        except Exception as e:
            error_msg = f"Error in analyze_model: {str(e)}"
            logger.exception(error_msg)
            return AgentResult(success=False, message=error_msg, error=str(e))

    async def arun(self, input_data: Dict[str, Any]) -> AgentResult:
        try:
            if "tool_results" not in input_data:
                input_data.update({"tool_results": self._current_tool_results, "tools": "\n".join(f"- {t.name}: {t.description}" for t in self.tools)})
            logger.info(f"Starting async analysis with input: {input_data}")
            result = await self._agent_executor.ainvoke(input_data)
            formatted_result = self._format_result(result)
            self._log_execution("complete", formatted_result.dict())
            return formatted_result
        except Exception as e:
            error_msg = f"Async agent execution failed: {str(e)}"
            self._log_execution("error", {"error": error_msg})
            logger.error(error_msg)
            return AgentResult(success=False, message=error_msg, error=str(e))

################################################################################
END OF FILE: src/agents/metabolic.py
################################################################################

================================================================================
FILE: src/config/__init__.py
================================================================================

from .settings import load_config, save_config, get_config, update_config, ConfigManager
from .prompts import load_prompts, get_prompt, PromptManager

__all__ = [
    'load_config',
    'save_config',
    'get_config',
    'update_config',
    'ConfigManager',
    'load_prompts',
    'get_prompt',
    'PromptManager'
]

################################################################################
END OF FILE: src/config/__init__.py
################################################################################

================================================================================
FILE: src/config/prompts.py
================================================================================

# src/config/prompts.py
from typing import Dict, Any, Optional, Union
import yaml
from pathlib import Path
import logging
from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate as LangChainPromptTemplate

logger = logging.getLogger(__name__)

class PromptTemplate(BaseModel):
    """Model for prompt templates"""
    name: str
    description: str
    template: str
    input_variables: list[str]
    metadata: Dict[str, Any] = Field(default_factory=dict)

class AgentPrompts(BaseModel):
    """Collection of prompts for an agent type"""
    prefix: str
    suffix: str
    format_instructions: str
    examples: Optional[list[Dict[str, str]]] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ToolPrompts(BaseModel):
    """Collection of prompts for tools"""
    description: str
    usage: str
    examples: Optional[list[Dict[str, str]]] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class Prompts(BaseModel):
    """Main prompts configuration"""
    agents: Dict[str, AgentPrompts]
    tools: Dict[str, ToolPrompts]
    templates: Dict[str, PromptTemplate]

class PromptManager:
    """Manager for handling prompt templates"""

    _instance = None
    _prompts: Optional[Prompts] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(PromptManager, cls).__new__(cls)
        return cls._instance

    @classmethod
    def load_prompts(cls, prompts_dir: Optional[Union[str, Path]] = None) -> Prompts:
        """
        Load prompts from YAML files in the prompts directory.

        Args:
            prompts_dir: Directory containing prompt YAML files

        Returns:
            Loaded prompts configuration

        Raises:
            FileNotFoundError: If prompts directory is not found
            ValueError: If prompts are invalid
        """
        if prompts_dir is None:
            prompts_dir = cls._find_prompts_dir()
        else:
            prompts_dir = Path(prompts_dir)

        try:
            # Load agent prompts
            agents_file = prompts_dir / "metabolic.yaml"  # Changed from agents.yaml
            with open(agents_file, 'r') as f:
                config = yaml.safe_load(f)

            # Extract sections
            agent_config = config.get('agent', {})
            tools_config = config.get('tools', {})
            templates_config = {}  # Will populate from agent config

            # Create template from agent prompts
            if 'prefix' in agent_config and 'format_instructions' in agent_config and 'suffix' in agent_config:
                templates_config['metabolic_agent'] = {
                    'name': 'metabolic_agent',
                    'description': config.get('description', ''),
                    'template': agent_config['prefix'] + '\n\n' + agent_config['format_instructions'] + '\n\n' + agent_config['suffix'],
                    'input_variables': ['input', 'agent_scratchpad', 'tool_results', 'tools']
                }

            prompts = Prompts(
                agents={
                    'metabolic': AgentPrompts(
                        prefix=agent_config.get('prefix', ''),
                        suffix=agent_config.get('suffix', ''),
                        format_instructions=agent_config.get('format_instructions', ''),
                        metadata=agent_config.get('metadata', {})
                    )
                },
                tools={k: ToolPrompts(**v) for k, v in tools_config.items()},
                templates={k: PromptTemplate(**v) for k, v in templates_config.items()}
            )

            cls._prompts = prompts
            logger.info(f"Successfully loaded prompts from {prompts_dir}")
            return prompts

        except Exception as e:
            logger.error(f"Error loading prompts: {str(e)}")
            raise

    @classmethod
    def _find_prompts_dir(cls) -> Path:
        """Find prompts directory in default locations"""
        search_paths = [
            Path.cwd() / 'config' / 'prompts',
            Path.home() / '.config' / 'metabolic_agent' / 'prompts',
            Path(__file__).parent.parent.parent / 'config' / 'prompts'
        ]

        for path in search_paths:
            if path.exists() and path.is_dir():
                return path

        raise FileNotFoundError(
            "Prompts directory not found in default locations: " +
            ", ".join(str(p) for p in search_paths)
        )

    @classmethod
    def get_prompts(cls) -> Prompts:
        """Get the current prompts configuration"""
        if cls._prompts is None:
            cls.load_prompts()
        return cls._prompts

    @classmethod
    def get_agent_prompt(cls, agent_type: str) -> AgentPrompts:
        """Get prompts for a specific agent type"""
        prompts = cls.get_prompts()
        if agent_type not in prompts.agents:
            raise ValueError(f"No prompts found for agent type: {agent_type}")
        return prompts.agents[agent_type]

def load_prompt_template(template_name: str) -> Optional[LangChainPromptTemplate]:
    """Load a prompt template and convert to LangChain format"""
    try:
        prompts = PromptManager.get_prompts()
        if template_name in prompts.templates:
            template = prompts.templates[template_name]
            return LangChainPromptTemplate(
                template=template.template,
                input_variables=template.input_variables
            )
        elif template_name == "metabolic":
            # Convert agent prompts to template
            agent_prompts = prompts.agents.get("metabolic")
            if agent_prompts:
                template = (
                    agent_prompts.prefix +
                    "\n\n" +
                    agent_prompts.format_instructions +
                    "\n\n" +
                    agent_prompts.suffix
                )
                return LangChainPromptTemplate(
                    template=template,
                    input_variables=["input", "agent_scratchpad", "tool_results", "tools"]
                )
        return None
    except Exception as e:
        logger.error(f"Error loading prompt template {template_name}: {e}")
        return None

# Convenience functions
def load_prompts(prompts_dir: Optional[Union[str, Path]] = None) -> Prompts:
    return PromptManager.load_prompts(prompts_dir)

def get_prompt(agent_type: str = None, tool_name: str = None, template_name: str = None) -> Union[AgentPrompts, ToolPrompts, PromptTemplate]:
    if agent_type:
        return PromptManager.get_agent_prompt(agent_type)
    elif template_name:
        prompts = PromptManager.get_prompts()
        return prompts.templates[template_name]
    else:
        raise ValueError("Must specify either agent_type or template_name")

################################################################################
END OF FILE: src/config/prompts.py
################################################################################

================================================================================
FILE: src/config/settings.py
================================================================================

# src/config/settings.py
import os
import yaml
from typing import Dict, Any, Optional, Union
from pathlib import Path
import logging
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

class LLMConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    llm_backend: str = "argo"
    safety_settings: Dict[str, Any] = Field(
        default_factory=lambda: {
            "enabled": True,
            "max_api_calls": 100,
            "max_tokens": 50000
        }
    )

class ArgoConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    user: str
    system_content: str
    models: Dict[str, Dict[str, str]]
    default_model: str = "gpt4"

class OpenAIConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    api_key: str
    api_name: str = "gpt-3.5-turbo"  # Changed from model_name
    system_content: str

class LocalModelConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    model_name: str
    model_path: str
    device: str = "mps"
    max_tokens: int = 500
    temperature: float = 0.7

class LocalLLMConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    model_name: str
    model_path: str
    system_content: str
    device: str = "mps"
    max_tokens: int = 500
    temperature: float = 0.7
    models: Dict[str, LocalModelConfig] = Field(default_factory=dict)

class ToolConfigs(BaseModel):
    model_config = {"protected_namespaces": ()}
    fba_config: Dict[str, Any] = Field(default_factory=dict)
    analysis_config: Dict[str, Any] = Field(default_factory=dict)

class ToolConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    configs: ToolConfigs

class AgentConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    max_iterations: int = 5
    verbose: bool = False
    handle_parsing_errors: bool = True
    additional_config: Dict[str, Any] = Field(default_factory=dict)

class Config(BaseModel):
    model_config = {"protected_namespaces": ()}
    llm: LLMConfig
    argo: Optional[ArgoConfig] = None
    openai: Optional[OpenAIConfig] = None
    local: Optional[LocalLLMConfig] = None
    tools: ToolConfig
    agent: AgentConfig

class ConfigManager:
    """Manager for handling configuration loading and access"""

    _instance = None
    _config: Optional[Config] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ConfigManager, cls).__new__(cls)
        return cls._instance

    @classmethod
    def load_config(cls, config_path: Optional[Union[str, Path]] = None) -> Config:
        """
        Load configuration from file.

        Args:
            config_path: Path to configuration file. If None, looks in default locations.

        Returns:
            Loaded configuration

        Raises:
            FileNotFoundError: If configuration file is not found
            ValueError: If configuration is invalid
        """
        if config_path is None:
            config_path = cls._find_config()

        try:
            with open(config_path, 'r') as f:
                raw_config = yaml.safe_load(f)

            # Process environment variables in config
            processed_config = cls._process_env_vars(raw_config)

            # Validate and create config object
            config = Config(**processed_config)
            cls._config = config

            logger.info(f"Successfully loaded configuration from {config_path}")
            return config

        except Exception as e:
            logger.error(f"Error loading configuration: {str(e)}")
            raise

    @classmethod
    def _find_config(cls) -> Path:
        """Find configuration file in default locations"""
        search_paths = [
            Path.cwd() / 'config' / 'config.yaml',
            Path.home() / '.config' / 'metabolic_agent' / 'config.yaml',
            Path(__file__).parent.parent.parent / 'config' / 'config.yaml'
        ]

        for path in search_paths:
            if path.exists():
                return path

        raise FileNotFoundError(
            "Configuration file not found in default locations: " +
            ", ".join(str(p) for p in search_paths)
        )

    @staticmethod
    def _process_env_vars(config: Dict[str, Any]) -> Dict[str, Any]:
        """Process environment variables in configuration"""
        def process_value(value):
            if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
                env_var = value[2:-1]
                return os.environ.get(env_var, value)
            elif isinstance(value, dict):
                return {k: process_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [process_value(v) for v in value]
            return value

        return process_value(config)

    @classmethod
    def get_config(cls) -> Config:
        """Get the current configuration"""
        if cls._config is None:
            cls.load_config()
        return cls._config

    @classmethod
    def save_config(cls, config: Config, path: Optional[Union[str, Path]] = None) -> None:
        """Save configuration to file"""
        if path is None:
            path = cls._find_config()

        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(path), exist_ok=True)

            # Convert config to dict and save
            config_dict = config.model_dump()
            with open(path, 'w') as f:
                yaml.safe_dump(config_dict, f, default_flow_style=False)

            logger.info(f"Successfully saved configuration to {path}")

        except Exception as e:
            logger.error(f"Error saving configuration: {str(e)}")
            raise

    @classmethod
    def update_config(cls, updates: Dict[str, Any]) -> Config:
        """Update current configuration with new values"""
        current_config = cls.get_config()
        config_dict = current_config.model_dump()

        def deep_update(d, u):
            for k, v in u.items():
                if isinstance(v, dict) and k in d:
                    d[k] = deep_update(d[k], v)
                else:
                    d[k] = v
            return d

        updated_dict = deep_update(config_dict, updates)
        cls._config = Config(**updated_dict)
        return cls._config

# Convenience functions
def load_config(config_path: Optional[Union[str, Path]] = None) -> Config:
    return ConfigManager.load_config(config_path)

def save_config(config: Config, path: Optional[Union[str, Path]] = None) -> None:
    ConfigManager.save_config(config, path)

def get_config() -> Config:
    return ConfigManager.get_config()

def update_config(updates: Dict[str, Any]) -> Config:
    return ConfigManager.update_config(updates)

################################################################################
END OF FILE: src/config/settings.py
################################################################################

================================================================================
FILE: src/llm/__init__.py
================================================================================

from .base import BaseLLM, LLMResponse, LLMConfig
from .factory import LLMFactory
from .argo import ArgoLLM
from .openai_llm import OpenAILLM

__all__ = [
    'BaseLLM',
    'LLMResponse',
    'LLMConfig',
    'ArgoLLM',
    'OpenAILLM',
    'LLMFactory'
]

################################################################################
END OF FILE: src/llm/__init__.py
################################################################################

================================================================================
FILE: src/llm/argo.py
================================================================================

# src/llm/argo.py
import requests
import httpx
import asyncio
from typing import Optional, List, Dict, Any
from langchain_core.messages import BaseMessage, AIMessage, SystemMessage, HumanMessage
from langchain_core.outputs import LLMResult
from .base import BaseLLM, LLMResponse

class ArgoLLM(BaseLLM):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._api_base = self._config.api_base
        self._user = self._config.user
        # Optional streaming API endpoint (if provided in the config)
        self._streaming_api = getattr(self._config, "streaming_api", None)

        if not self._api_base or not self._user:
            raise ValueError("ArgoLLM requires api_base and user to be set in config")

    @property
    def api_base(self):
        return self._api_base

    @property
    def user(self):
        return self._user

    def _format_messages(self, messages: List[BaseMessage]) -> Dict[str, Any]:
        prompt = []
        system_message = None

        for message in messages:
            if isinstance(message, SystemMessage):
                system_message = message.content
            elif isinstance(message, HumanMessage):
                prompt.append(message.content)
            elif isinstance(message, AIMessage):
                prompt.append(message.content)

        return {
            "prompt": prompt,
            "system": system_message or self.config.system_content
        }

    def _get_prompt_str(self, prompt: Any) -> str:
        if hasattr(prompt, "to_string"):
            return prompt.to_string()
        elif hasattr(prompt, "text"):
            return prompt.text
        elif isinstance(prompt, str):
            return prompt
        elif isinstance(prompt, list):
            return " ".join(str(p) for p in prompt)
        else:
            return str(prompt)

    def _generate_response(self,
                           prompt: Any,
                           max_tokens: Optional[int] = None,
                           temperature: Optional[float] = None,
                           stop_sequences: Optional[List[str]] = None,
                           stream: bool = False,
                           **kwargs) -> LLMResponse:
        # Convert prompt to string format
        if isinstance(prompt, list) and all(isinstance(m, BaseMessage) for m in prompt):
            formatted = self._format_messages(prompt)
            prompt_text = formatted["prompt"]
            system = formatted["system"]
        else:
            prompt_text = [self._get_prompt_str(prompt)]
            system = self.config.system_content

        estimated_tokens = self.estimate_tokens(str(prompt_text))
        self.check_limits(estimated_tokens)

        payload = {
            "user": self.user,
            "model": self.config.llm_name,
            "prompt": prompt_text,
            "system": system,
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens or 1000,
            "stop": stop_sequences or self.config.stop_sequences or []
        }

        # Special handling for o1 models: only use 'prompt' and 'max_completion_tokens'
        if self.config.llm_name in ["gpto1preview", "gpto1mini"]:
            payload["max_completion_tokens"] = payload.pop("max_tokens")
            payload.pop("system", None)
            payload.pop("stop", None)
            payload.pop("temperature", None)
            payload.pop("top_p", None)

        if not stream:
            # Synchronous call using requests
            response = requests.post(self.api_base, json=payload, headers={'Content-Type': 'application/json'})
            response.raise_for_status()
            data = response.json()
            reply = data.get('response', '')
            if not reply:
                raise ValueError("No response received from Argo API")
            tokens_used = data.get('usage', {}).get('total_tokens', estimated_tokens)
            self.update_usage(tokens_used)
            return LLMResponse(
                text=reply,
                tokens_used=tokens_used,
                llm_name=self.config.llm_name,
                metadata={"api_response": data}
            )
        else:
            # Streaming call using httpx asynchronous client
            if not self._streaming_api:
                raise ValueError("Streaming API endpoint not configured in the config")
            async def stream_response() -> LLMResponse:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    async with client.stream("POST", self._streaming_api, json=payload, headers={'Content-Type': 'application/json'}) as resp:
                        resp.raise_for_status()
                        collected_response = ""
                        async for chunk in resp.aiter_text():
                            collected_response += chunk
                        # A simple token estimation based on word count; adjust as needed
                        tokens_used = len(collected_response.split())
                        self.update_usage(tokens_used)
                        return LLMResponse(
                            text=collected_response,
                            tokens_used=tokens_used,
                            llm_name=self.config.llm_name,
                            metadata={"streamed": True}
                        )
            return asyncio.run(stream_response())

    # Asynchronous version of _generate_response for async workflows
    async def a_generate_response(self,
                                  prompt: Any,
                                  max_tokens: Optional[int] = None,
                                  temperature: Optional[float] = None,
                                  stop_sequences: Optional[List[str]] = None,
                                  stream: bool = False,
                                  **kwargs) -> LLMResponse:
        if isinstance(prompt, list) and all(isinstance(m, BaseMessage) for m in prompt):
            formatted = self._format_messages(prompt)
            prompt_text = formatted["prompt"]
            system = formatted["system"]
        else:
            prompt_text = [self._get_prompt_str(prompt)]
            system = self.config.system_content

        estimated_tokens = self.estimate_tokens(str(prompt_text))
        self.check_limits(estimated_tokens)

        payload = {
            "user": self.user,
            "model": self.config.llm_name,
            "prompt": prompt_text,
            "system": system,
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens or 1000,
            "stop": stop_sequences or self.config.stop_sequences or []
        }

        if self.config.llm_name in ["gpto1preview", "gpto1mini"]:
            payload["max_completion_tokens"] = payload.pop("max_tokens")
            payload.pop("system", None)
            payload.pop("stop", None)
            payload.pop("temperature", None)
            payload.pop("top_p", None)

        if not stream:
            async with httpx.AsyncClient(timeout=60.0) as client:
                resp = await client.post(self.api_base, json=payload, headers={'Content-Type': 'application/json'})
                resp.raise_for_status()
                data = resp.json()
                reply = data.get('response', '')
                if not reply:
                    raise ValueError("No response received from Argo API")
                tokens_used = data.get('usage', {}).get('total_tokens', estimated_tokens)
                self.update_usage(tokens_used)
                return LLMResponse(
                    text=reply,
                    tokens_used=tokens_used,
                    llm_name=self.config.llm_name,
                    metadata={"api_response": data}
                )
        else:
            if not self._streaming_api:
                raise ValueError("Streaming API endpoint not configured in the config")
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream("POST", self._streaming_api, json=payload, headers={'Content-Type': 'application/json'}) as resp:
                    resp.raise_for_status()
                    collected_response = ""
                    async for chunk in resp.aiter_text():
                        collected_response += chunk
                    tokens_used = len(collected_response.split())
                    self.update_usage(tokens_used)
                    return LLMResponse(
                        text=collected_response,
                        tokens_used=tokens_used,
                        llm_name=self.config.llm_name,
                        metadata={"streamed": True}
                    )

################################################################################
END OF FILE: src/llm/argo.py
################################################################################

================================================================================
FILE: src/llm/base.py
================================================================================

from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any, Sequence
from pydantic import BaseModel, Field, ConfigDict
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.llms import BaseLLM as LangchainBaseLLM
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.outputs import Generation, LLMResult
from dataclasses import dataclass

@dataclass
class LLMConfig:
    llm_name: str
    system_content: str
    max_tokens: Optional[int]
    temperature: float = 0.7
    stop_sequences: Optional[List[str]] = None
    api_base: Optional[str] = None
    user: Optional[str] = None
    safety_settings: Dict[str, Any] = Field(default_factory=lambda: {
        "enabled": True,
        "max_api_calls": 100,
        "max_tokens": 50000
    })

class LLMResponse(BaseModel):
    text: str
    tokens_used: int
    llm_name: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    model_config = ConfigDict(protected_namespaces=())

class BaseLLM(LangchainBaseLLM):
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self._tokens = 0
        self._calls = 0
        self._config = self._create_config(config)

    def _create_config(self, config: Dict[str, Any]) -> LLMConfig:
        safety_settings = config.get('safety_settings', {
            "enabled": True,
            "max_api_calls": 100,
            "max_tokens": 50000
        })

        return LLMConfig(
            llm_name=config['llm_name'],
            system_content=config['system_content'],
            max_tokens=config.get('max_tokens'),
            temperature=config.get('temperature', 0.7),
            stop_sequences=config.get('stop_sequences'),
            api_base=config.get('api_base'),
            user=config.get('user'),
            safety_settings=safety_settings
        )

    @property
    def config(self):
        return self._config

    def update_usage(self, tokens: int):
        self._tokens += tokens
        self._calls += 1

    def check_limits(self, estimated_tokens: int):
        if self.config.safety_settings["enabled"]:
            max_tokens = self.config.safety_settings["max_tokens"]
            max_calls = self.config.safety_settings["max_api_calls"]

            if self._tokens + estimated_tokens > max_tokens:
                raise ValueError(f"Token limit exceeded. Current: {self._tokens}, Requested: {estimated_tokens}")
            if self._calls + 1 > max_calls:
                raise ValueError(f"API call limit exceeded. Current: {self._calls}")

    @abstractmethod
    def _generate_response(self,
                        prompt: str,
                        max_tokens: Optional[int] = None,
                        temperature: Optional[float] = None,
                        stop_sequences: Optional[List[str]] = None) -> LLMResponse:
        pass

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        callbacks: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        generations = []
        for prompt in prompts:
            response = self._generate_response(prompt, stop_sequences=stop, **kwargs)
            generations.append([Generation(text=response.text)])
        return LLMResult(generations=generations)

    def estimate_tokens(self, text: str) -> int:
        return max(1, len(text) // 4)

    def _format_messages_as_text(self, messages: Sequence[BaseMessage]) -> str:
        message_text = []
        for message in messages:
            role = message.__class__.__name__.replace('Message', '')
            message_text.append(f"{role}: {message.content}")
        return "\n".join(message_text)

    def predict(self, text: str, **kwargs: Any) -> str:
        response = self._generate_response(text, **kwargs)
        return response.text

    def predict_messages(self, messages: List[BaseMessage], **kwargs: Any) -> BaseMessage:
        text = self._format_messages_as_text(messages)
        response = self._generate_response(text, **kwargs)
        return AIMessage(content=response.text)

    def invoke(self, input: Any, config: Optional[dict] = None, **kwargs) -> AIMessage:
        if isinstance(input, dict):
            text = input.get('input', '')
            if isinstance(text, (list, tuple)):
                text = self._format_messages_as_text(text)
            response = self.predict(str(text), **kwargs)
            return AIMessage(content=response)

        if isinstance(input, (list, tuple)):
            text = self._format_messages_as_text(input)
        else:
            text = str(input)
        response = self.predict(text, **kwargs)
        return AIMessage(content=response)

    async def ainvoke(self, input: Any, config: Optional[dict] = None, **kwargs) -> AIMessage:
        return self.invoke(input, config, **kwargs)

    async def apredict(self, text: str, **kwargs) -> str:
        return self.predict(text, **kwargs)

    async def apredict_messages(self, messages: List[BaseMessage], **kwargs) -> BaseMessage:
        return self.predict_messages(messages, **kwargs)

    @property
    def _llm_type(self) -> str:
        return self.__class__.__name__.lower().replace('llm', '')

################################################################################
END OF FILE: src/llm/base.py
################################################################################

================================================================================
FILE: src/llm/factory.py
================================================================================

# src/llm/factory.py
from typing import Dict, Any, Type
from .base import BaseLLM
from .argo import ArgoLLM
from .openai_llm import OpenAILLM

class LLMFactory:
    @classmethod
    def _get_llm_class(cls, backend: str) -> Type[BaseLLM]:
        """Get the LLM class based on backend type with lazy loading"""
        if backend == "argo":
            return ArgoLLM
        elif backend == "openai":
            return OpenAILLM
        elif backend == "local":
            # Import LocalLLM only when needed to avoid import errors
            from .local_llm import LocalLLM
            return LocalLLM
        else:
            raise ValueError(
                f"Unsupported LLM backend: {backend}. "
                f"Available backends: {cls.list_backends()}"
            )

    @classmethod
    def create(cls, backend: str, config: Dict[str, Any]) -> BaseLLM:
        """Create an LLM instance based on the specified backend"""
        try:
            llm_class = cls._get_llm_class(backend)
            return llm_class(config)
        except Exception as e:
            raise ValueError(f"Failed to create {backend} LLM: {str(e)}")

    @classmethod
    def list_backends(cls) -> list:
        """List all registered LLM backends"""
        return ["argo", "openai", "local"]

################################################################################
END OF FILE: src/llm/factory.py
################################################################################

================================================================================
FILE: src/llm/local_llm.py
================================================================================

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional, List, Dict, Any
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.outputs import LLMResult
from .base import BaseLLM, LLMResponse

class LocalLLM(BaseLLM):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        if 'path' not in config:
            raise ValueError("LocalLLM requires 'path' in config")

        self._model_path = config['path']
        self._device = torch.device(config.get('device',
                                  "mps" if torch.backends.mps.is_available()
                                  else "cuda" if torch.cuda.is_available()
                                  else "cpu"))
        self._tokenizer = None
        self._model = None
        self._load_model()

    def _load_model(self) -> None:
        """Load the model using Hugging Face auto classes"""
        try:
            # Load tokenizer
            self._tokenizer = AutoTokenizer.from_pretrained(
                self._model_path,
                trust_remote_code=True
            )

            # Load model with optimizations
            self._model = AutoModelForCausalLM.from_pretrained(
                self._model_path,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            ).to(self._device)

            # Set to evaluation mode
            self._model.eval()

        except Exception as e:
            raise ValueError(f"Failed to load model from {self._model_path}: {str(e)}")

    def _generate_response(self,
                          prompt: str,
                          max_tokens: Optional[int] = None,
                          temperature: Optional[float] = None,
                          stop_sequences: Optional[List[str]] = None,
                          stop: Optional[List[str]] = None,
                          **kwargs) -> LLMResponse:
        """Generate a response from the model"""
        if self._model is None or self._tokenizer is None:
            raise ValueError("Model and tokenizer must be loaded before generating responses")

        full_prompt = f"{self.config.system_content}\nUser: {prompt}\nAssistant:"

        try:
            inputs = self._tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to(self._device)

            generation_kwargs = {
                "max_new_tokens": max_tokens or self.config.max_tokens or 100,
                "do_sample": True,
                "temperature": temperature or self.config.temperature,
                "pad_token_id": self._tokenizer.eos_token_id,
                "num_return_sequences": 1
            }

            # Add stop sequences if provided
            if stop or stop_sequences:
                all_stops = []
                if stop:
                    all_stops.extend(stop)
                if stop_sequences:
                    all_stops.extend(stop_sequences)
                generation_kwargs["stopping_criteria"] = all_stops

            with torch.no_grad():
                outputs = self._model.generate(
                    **inputs,
                    **generation_kwargs
                )

            response_text = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_reply = response_text.split('Assistant:')[-1].strip()

            tokens_used = len(outputs[0])
            self.update_usage(tokens_used)

            return LLMResponse(
                text=assistant_reply,
                tokens_used=tokens_used,
                llm_name=self.config.llm_name,
                metadata={
                    "model_path": self._model_path,
                    "device": str(self._device)
                }
            )

        except Exception as e:
            raise ValueError(f"Local LLM Error: {str(e)}")

    def predict(self, text: str, **kwargs) -> str:
        """Predict response for given text"""
        response = self._generate_response(text, **kwargs)
        return response.text

    def predict_messages(self, messages: List[BaseMessage], **kwargs) -> BaseMessage:
        """Predict response for given messages"""
        text = self._format_messages_as_text(messages)
        response = self._generate_response(text, **kwargs)
        return AIMessage(content=response.text)

    def generate_prompt(self, prompts: List[str], **kwargs) -> LLMResult:
        """Generate responses for multiple prompts"""
        results = [self._generate_response(prompt, **kwargs).text for prompt in prompts]
        return LLMResult(generations=[[{"text": text}] for text in results])

    async def agenerate_prompt(self, prompts: List[str], **kwargs) -> LLMResult:
        """Async version of generate_prompt"""
        return self.generate_prompt(prompts, **kwargs)

    async def apredict(self, text: str, **kwargs) -> str:
        """Async version of predict"""
        return self.predict(text, **kwargs)

    async def apredict_messages(self, messages: List[BaseMessage], **kwargs) -> BaseMessage:
        """Async version of predict_messages"""
        return self.predict_messages(messages, **kwargs)

    def __del__(self):
        """Cleanup method"""
        if hasattr(self, '_model') and self._model is not None:
            try:
                del self._model
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
            except:
                pass

################################################################################
END OF FILE: src/llm/local_llm.py
################################################################################

================================================================================
FILE: src/llm/openai_llm.py
================================================================================

# src/llm/openai_llm.py
import openai
from typing import Optional, List, Dict, Any
from langchain_core.messages import BaseMessage, AIMessage
from langchain_core.outputs import LLMResult
from .base import BaseLLM, LLMResponse, LLMConfig

class OpenAILLMConfig(LLMConfig):
    api_key: str
    class Config:
        protected_namespaces = ()

class OpenAILLM(BaseLLM):
    def __init__(self, config: Dict[str, Any]):
        super().__init__(OpenAILLMConfig(**config))
        openai.api_key = self.config.api_key

    def _generate_response(self,
                          prompt: str,
                          max_tokens: Optional[int] = None,
                          temperature: Optional[float] = None,
                          stop_sequences: Optional[List[str]] = None) -> LLMResponse:
        estimated_tokens = self.estimate_tokens(prompt)
        self.check_limits(estimated_tokens)

        messages = [
            {"role": "system", "content": self.config.system_content},
            {"role": "user", "content": prompt}
        ]

        try:
            response = openai.ChatCompletion.create(
                model=self.config.llm_name,
                messages=messages,
                temperature=temperature or self.config.temperature,
                max_tokens=max_tokens or self.config.max_tokens,
                stop=stop_sequences or self.config.stop_sequences
            )

            reply = response["choices"][0]["message"]["content"]
            tokens_used = response['usage']['total_tokens']

            self.update_usage(tokens_used)

            return LLMResponse(
                text=reply,
                tokens_used=tokens_used,
                llm_name=self.config.llm_name,
                metadata={"api_response": response}
            )

        except Exception as e:
            raise ValueError(f"OpenAI API Error: {str(e)}")

    def invoke(self, input: Any, config: Optional[dict] = None, **kwargs) -> AIMessage:
        if isinstance(input, dict):
            text = input.get('input', '')
            if isinstance(text, (list, tuple)):
                text = self._format_messages_as_text(text)
            response = self.predict(str(text), **kwargs)
            return AIMessage(content=response)

        text = self._format_messages_as_text(input)
        response = self.predict(text, **kwargs)
        return AIMessage(content=response)

    async def agenerate_prompt(self, prompts: List[str], **kwargs) -> LLMResult:
        return self.generate_prompt(prompts, **kwargs)

    async def apredict(self, text: str, **kwargs) -> str:
        return self.predict(text, **kwargs)

    async def apredict_messages(self, messages: List[BaseMessage], **kwargs) -> BaseMessage:
        return self.predict_messages(messages, **kwargs)

    async def ainvoke(self, input: Any, config: Optional[dict] = None, **kwargs) -> AIMessage:
        return await self.invoke(input, config, **kwargs)

################################################################################
END OF FILE: src/llm/openai_llm.py
################################################################################

================================================================================
FILE: src/tools/__init__.py
================================================================================

from .base import BaseTool, ToolResult, ToolRegistry
from .cobra.fba import FBATool
from .cobra.analysis import ModelAnalysisTool, PathwayAnalysisTool
from .cobra.utils import ModelUtils

__all__ = [
    'BaseTool',
    'ToolResult',
    'ToolRegistry',
    'FBATool',
    'ModelAnalysisTool',
    'PathwayAnalysisTool',
    'ModelUtils'
]

################################################################################
END OF FILE: src/tools/__init__.py
################################################################################

================================================================================
FILE: src/tools/base.py
================================================================================

from typing import Optional, Dict, Any, List, ClassVar
from pydantic import BaseModel, Field
from langchain_core.tools import BaseTool as LangChainBaseTool

class ToolResult(BaseModel):
    model_config = {"protected_namespaces": ()}
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    error: Optional[str] = None

class BaseTool(LangChainBaseTool):
    """Base class for all tools"""
    tool_name: ClassVar[str] = ""
    tool_description: ClassVar[str] = ""

    def __init__(self, config: Dict[str, Any]):
        # Initialize the parent class first
        super().__init__(
            name=config.get("name", self.tool_name),
            description=config.get("description", self.tool_description)
        )
        # Store the configuration
        self._config = config

    @property
    def tool_config(self) -> Dict[str, Any]:
        """Get the tool configuration"""
        return self._config

class ToolRegistry:
    _tools: Dict[str, type] = {}

    @classmethod
    def register(cls, tool_class: type):
        if not issubclass(tool_class, BaseTool):
            raise ValueError(f"Tool class must inherit from BaseTool: {tool_class}")
        cls._tools[tool_class.tool_name] = tool_class
        return tool_class

    @classmethod
    def get_tool(cls, name: str) -> Optional[type]:
        return cls._tools.get(name)

    @classmethod
    def create_tool(cls, name: str, config: Dict[str, Any]) -> BaseTool:
        tool_class = cls.get_tool(name)
        if tool_class is None:
            raise ValueError(f"Tool not found: {name}")
        return tool_class(config)

    @classmethod
    def list_tools(cls) -> List[str]:
        return list(cls._tools.keys())

################################################################################
END OF FILE: src/tools/base.py
################################################################################

================================================================================
FILE: src/tools/cobra/__init__.py
================================================================================

from .fba import FBATool
from .analysis import ModelAnalysisTool, PathwayAnalysisTool
from .utils import ModelUtils

__all__ = [
    'FBATool',
    'ModelAnalysisTool',
    'PathwayAnalysisTool',
    'ModelUtils'
]

################################################################################
END OF FILE: src/tools/cobra/__init__.py
################################################################################

================================================================================
FILE: src/tools/cobra/analysis.py
================================================================================

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field, PrivateAttr
import cobra
from ..base import BaseTool, ToolResult, ToolRegistry
from .utils import ModelUtils

class ModelAnalysisConfig(BaseModel):
    """Configuration for model analysis tool"""
    model_config = {"protected_namespaces": ()}
    flux_threshold: float = 1e-6
    include_reactions: Optional[List[str]] = None
    include_subsystems: Optional[bool] = True
    track_metabolites: Optional[bool] = True

@ToolRegistry.register
class ModelAnalysisTool(BaseTool):
    """Tool for analyzing metabolic model properties"""

    tool_name = "analyze_metabolic_model"
    tool_description = """Analyze structural properties of a metabolic model including
    reaction connectivity, pathway completeness, and potential gaps."""

    _analysis_config: ModelAnalysisConfig = PrivateAttr()
    _utils: ModelUtils = PrivateAttr()

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # Extract analysis_config from tool_config
        analysis_config_dict = config.get("analysis_config", {})
        if isinstance(analysis_config_dict, dict):
            self._analysis_config = ModelAnalysisConfig(**analysis_config_dict)
        else:
            # If analysis_config is already a Pydantic model or similar
            self._analysis_config = ModelAnalysisConfig(
                flux_threshold=getattr(analysis_config_dict, "flux_threshold", 1e-6),
                include_reactions=getattr(analysis_config_dict, "include_reactions", None),
                include_subsystems=getattr(analysis_config_dict, "include_subsystems", True),
                track_metabolites=getattr(analysis_config_dict, "track_metabolites", True)
            )
        self._utils = ModelUtils()

    @property
    def analysis_config(self) -> ModelAnalysisConfig:
        return self._analysis_config

    def _run(self, model_path: str) -> ToolResult:
        try:
            # Load model
            model = self._utils.load_model(model_path)

            # Analyze model structure
            analysis_results = {
                "model_statistics": self._get_basic_statistics(model),
                "network_properties": self._analyze_network_properties(model),
                "subsystem_analysis": self._analyze_subsystems(model),
                "potential_issues": self._identify_model_issues(model)
            }

            return ToolResult(
                success=True,
                message="Model analysis completed successfully",
                data=analysis_results
            )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error analyzing model",
                error=str(e)
            )

    def _get_basic_statistics(self, model: cobra.Model) -> Dict[str, Any]:
        """Get basic model statistics"""
        return {
            "num_reactions": len(model.reactions),
            "num_metabolites": len(model.metabolites),
            "num_genes": len(model.genes),
            "num_subsystems": len(set(rxn.subsystem for rxn in model.reactions if rxn.subsystem))
        }

    def _analyze_network_properties(self, model: cobra.Model) -> Dict[str, Any]:
        """Analyze network properties with improved metrics"""
        # Initialize statistics
        network_stats = {
            "connectivity_summary": {
                "total_connections": 0,
                "avg_connections_per_metabolite": 0,
                "max_connections": 0,
                "min_connections": float('inf')
            },
            "highly_connected_metabolites": [],
            "isolated_metabolites": [],
            "choke_points": []  # Metabolites that are sole producers/consumers
        }

        # Analyze each metabolite
        for metabolite in model.metabolites:
            num_reactions = len(metabolite.reactions)
            producing = [r for r in metabolite.reactions if metabolite in r.products]
            consuming = [r for r in metabolite.reactions if metabolite in r.reactants]

            # Update summary statistics
            network_stats["connectivity_summary"]["total_connections"] += num_reactions
            network_stats["connectivity_summary"]["max_connections"] = max(
                network_stats["connectivity_summary"]["max_connections"],
                num_reactions
            )
            network_stats["connectivity_summary"]["min_connections"] = min(
                network_stats["connectivity_summary"]["min_connections"],
                num_reactions
            )

            # Track highly connected metabolites (hub metabolites)
            if num_reactions > 10:
                network_stats["highly_connected_metabolites"].append({
                    "id": metabolite.id,
                    "name": metabolite.name,
                    "num_reactions": num_reactions,
                    "num_producing": len(producing),
                    "num_consuming": len(consuming)
                })

            # Track isolated metabolites
            if num_reactions <= 1:
                network_stats["isolated_metabolites"].append({
                    "id": metabolite.id,
                    "name": metabolite.name,
                    "num_reactions": num_reactions
                })

            # Identify choke points
            if len(producing) == 1 or len(consuming) == 1:
                network_stats["choke_points"].append({
                    "id": metabolite.id,
                    "name": metabolite.name,
                    "single_producer": len(producing) == 1,
                    "single_consumer": len(consuming) == 1
                })

        # Calculate average connectivity
        num_metabolites = len(model.metabolites)
        if num_metabolites > 0:
            network_stats["connectivity_summary"]["avg_connections_per_metabolite"] = (
                network_stats["connectivity_summary"]["total_connections"] / num_metabolites
            )

        # Sort and limit lists
        network_stats["highly_connected_metabolites"].sort(
            key=lambda x: x["num_reactions"],
            reverse=True
        )
        network_stats["highly_connected_metabolites"] = (
            network_stats["highly_connected_metabolites"][:10]  # Top 10 most connected
        )

        return network_stats

    def _analyze_subsystems(self, model: cobra.Model) -> Dict[str, Any]:
        """Analyze subsystem properties"""
        subsystems = {}
        orphan_reactions = []

        for reaction in model.reactions:
            subsystem = reaction.subsystem or "Unknown"

            if subsystem not in subsystems:
                subsystems[subsystem] = {
                    "reaction_count": 0,
                    "gene_associated": 0,
                    "spontaneous": 0,
                    "reversible": 0
                }

            stats = subsystems[subsystem]
            stats["reaction_count"] += 1

            if reaction.genes:
                stats["gene_associated"] += 1
            else:
                stats["spontaneous"] += 1

            if reaction.reversibility:
                stats["reversible"] += 1

            if not subsystem or subsystem == "Unknown":
                orphan_reactions.append({
                    "id": reaction.id,
                    "name": reaction.name,
                    "reversible": reaction.reversibility
                })

        return {
            "subsystem_statistics": subsystems,
            "orphan_reactions": orphan_reactions,
            "num_subsystems": len(subsystems),
            "largest_subsystems": sorted(
                [(k, v["reaction_count"]) for k, v in subsystems.items()],
                key=lambda x: x[1],
                reverse=True
            )[:5]  # Top 5 largest subsystems
        }

    def _identify_model_issues(self, model: cobra.Model) -> Dict[str, Any]:
        """Identify potential issues in the model"""
        issues = {
            "dead_end_metabolites": [],
            "disconnected_reactions": [],
            "missing_genes": [],
            "boundary_issues": []
        }

        # Find dead-end metabolites
        for metabolite in model.metabolites:
            if len(metabolite.reactions) <= 1:
                issues["dead_end_metabolites"].append({
                    "id": metabolite.id,
                    "name": metabolite.name,
                    "connected_reactions": [r.id for r in metabolite.reactions]
                })

        # Find disconnected reactions
        for reaction in model.reactions:
            if len(reaction.metabolites) == 0:
                issues["disconnected_reactions"].append(reaction.id)

        # Check for reactions without genes (excluding exchanges/demands)
        for reaction in model.reactions:
            if not reaction.genes and not (
                reaction.id.startswith("EX_") or
                reaction.id.startswith("DM_") or
                reaction.id.startswith("SK_")
            ):
                issues["missing_genes"].append(reaction.id)

        # Check boundary conditions
        boundary_metabolites = set()
        for reaction in model.reactions:
            if len(reaction.metabolites) == 1:
                boundary_metabolites.update(reaction.metabolites)

        issues["boundary_issues"] = [{
            "metabolite": met.id,
            "name": met.name
        } for met in boundary_metabolites]

        return issues

@ToolRegistry.register
class PathwayAnalysisTool(BaseTool):
    """Tool for analyzing specific metabolic pathways"""
    tool_name = "analyze_pathway"
    tool_description = """Analyze specific metabolic pathways including flux distributions,
    gene associations, and regulatory features."""

    _utils: ModelUtils = PrivateAttr()

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._utils = ModelUtils()

    def _run(self, input_data: Dict[str, Any]) -> ToolResult:
        try:
            model_path = input_data.get("model_path")
            pathway = input_data.get("pathway")
            if not model_path or not pathway:
                raise ValueError("Both model_path and pathway must be provided")

            model = self._utils.load_model(model_path)
            pathway_reactions = [
                rxn for rxn in model.reactions
                if rxn.subsystem and pathway.lower() in rxn.subsystem.lower()
            ]

            if not pathway_reactions:
                return ToolResult(
                    success=False,
                    message=f"No reactions found for pathway: {pathway}",
                    error="Pathway not found"
                )

            pathway_analysis = {
                "summary": {
                    "reaction_count": len(pathway_reactions),
                    "gene_coverage": len(set(gene for rxn in pathway_reactions for gene in rxn.genes)),
                    "metabolite_count": len(set(met for rxn in pathway_reactions for met in rxn.metabolites)),
                    "reversible_reactions": sum(1 for rxn in pathway_reactions if rxn.reversibility)
                },
                "reactions": [
                    {
                        "id": rxn.id,
                        "name": rxn.name,
                        "reaction": rxn.build_reaction_string(),
                        "genes": [gene.id for gene in rxn.genes],
                        "metabolites": [met.id for met in rxn.metabolites],
                        "bounds": rxn.bounds
                    }
                    for rxn in pathway_reactions
                ],
                "connectivity": {
                    "input_metabolites": list(set(
                        met.id for rxn in pathway_reactions
                        for met in rxn.reactants
                        if met not in set(m for r in pathway_reactions for m in r.products)
                    )),
                    "output_metabolites": list(set(
                        met.id for rxn in pathway_reactions
                        for met in rxn.products
                        if met not in set(m for r in pathway_reactions for m in r.reactants)
                    ))
                }
            }

            return ToolResult(
                success=True,
                message=f"Pathway analysis completed for: {pathway}",
                data=pathway_analysis
            )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error analyzing pathway",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/cobra/analysis.py
################################################################################

================================================================================
FILE: src/tools/cobra/auxotrophy.py
================================================================================

from typing import Dict, Any
from pydantic import BaseModel, Field, PrivateAttr
import cobra
from ..base import BaseTool, ToolResult, ToolRegistry
from .utils import ModelUtils
from .simulation_wrapper import run_simulation
from .fba import SimulationResultsStore  # Import the results store

class AuxotrophyConfig(BaseModel):
    """Configuration for auxotrophy identification tool."""
    growth_threshold: float = 1e-6
    candidate_metabolites: list = Field(default_factory=lambda: [
        "EX_arg__L_e", "EX_leu__L_e", "EX_lys__L_e"
    ])

@ToolRegistry.register
class AuxotrophyTool(BaseTool):
    """Tool to identify potential auxotrophies by testing the removal of candidate nutrients using standard FBA."""
    tool_name = "identify_auxotrophies"
    tool_description = "Identify potential auxotrophies by testing the removal of candidate nutrients using FBA."

    _config: AuxotrophyConfig

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        config_dict = config.get("auxotrophy_config", {})
        self._config = AuxotrophyConfig(**config_dict)
        self._utils = ModelUtils()

    def _run(self, input_data: Any) -> ToolResult:
        try:
            if isinstance(input_data, dict):
                model_path = input_data.get("model_path")
                output_dir = input_data.get("output_dir", None)
            else:
                model_path = input_data
                output_dir = None

            model = self._utils.load_model(model_path)
            complete_solution = run_simulation(model, method="fba")
            complete_growth = complete_solution.objective_value

            # Optionally export the complete simulation results
            result_file = None
            if output_dir:
                store = SimulationResultsStore()
                result_id = store.save_results(self.tool_name, model.id, complete_solution, additional_metadata={"simulation_method": "fba"})
                json_path, csv_path = store.export_results(result_id, output_dir)
                result_file = {"json": json_path, "csv": csv_path}

            auxotrophies = []
            for met_id in self._config.candidate_metabolites:
                try:
                    rxn = model.reactions.get_by_id(met_id)
                    original_bounds = rxn.bounds
                    rxn.lower_bound = 0
                    test_solution = run_simulation(model, method="fba")
                    rxn.bounds = original_bounds
                    if test_solution.objective_value < self._config.growth_threshold:
                        auxotrophies.append(met_id)
                except Exception:
                    continue
            return ToolResult(
                success=True,
                message="Auxotrophy analysis completed.",
                data={"complete_growth_rate": complete_growth, "auxotrophies": auxotrophies, "result_file": result_file}
            )
        except Exception as e:
            return ToolResult(
                success=False,
                message="Error identifying auxotrophies.",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/cobra/auxotrophy.py
################################################################################

================================================================================
FILE: src/tools/cobra/fba.py
================================================================================

from typing import Dict, Any
from pydantic import BaseModel, Field, PrivateAttr
import os
import json
import pandas as pd
from datetime import datetime
import cobra
from cobra.io import read_sbml_model
from ..base import BaseTool, ToolResult, ToolRegistry
from .utils import ModelUtils
from .simulation_wrapper import run_simulation

# Configuration for FBA tool
class FBAConfig(BaseModel):
    """Configuration for FBA tool"""
    model_config = {"protected_namespaces": ()}
    default_objective: str = "biomass_reaction"
    solver: str = "glpk"
    tolerance: float = 1e-6
    additional_constraints: Dict[str, float] = Field(default_factory=dict)
    simulation_method: str = "pfba"  # Options: "fba", "pfba", "geometric", "slim", etc.

# --------------------------
# Simulation Results Store
# --------------------------
class SimulationResultsStore:
    """
    A simple store to save simulation results and export them to JSON and CSV.
    """
    def __init__(self):
        self.results = {}

    def save_results(self, tool_name: str, model_id: str, solution: cobra.Solution, additional_metadata=None) -> str:
        """Store simulation results with metadata and return a unique result ID."""
        timestamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
        result_id = f"{tool_name}_{model_id}_{timestamp}"
        result_data = {
            "objective_value": float(solution.objective_value),
            "fluxes": solution.fluxes.to_dict(),
            "status": solution.status,
            "timestamp": timestamp,
            "model_id": model_id,
            "tool": tool_name,
            "metadata": additional_metadata or {}
        }
        self.results[result_id] = result_data
        return result_id

    def export_results(self, result_id: str, output_dir: str):
        """Export simulation results to JSON and CSV files in the output directory."""
        if result_id not in self.results:
            return None, None
        os.makedirs(output_dir, exist_ok=True)
        result = self.results[result_id]
        json_path = os.path.join(output_dir, f"{result_id}.json")
        with open(json_path, 'w') as f:
            json.dump(result, f, indent=2)
        flux_df = pd.DataFrame.from_dict(result["fluxes"], orient='index', columns=['flux'])
        csv_path = os.path.join(output_dir, f"{result_id}_fluxes.csv")
        flux_df.to_csv(csv_path)
        return json_path, csv_path

# --------------------------
# Updated FBATool Implementation
# --------------------------
@ToolRegistry.register
class FBATool(BaseTool):
    """Tool for running Flux Balance Analysis with configurable simulation method and result export."""

    tool_name = "run_metabolic_fba"
    tool_description = "Run Flux Balance Analysis (FBA) on a metabolic model using a configurable simulation method."

    _fba_config: FBAConfig = PrivateAttr()
    _utils: ModelUtils = PrivateAttr()

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        fba_config_dict = config.get("fba_config", {})
        if isinstance(fba_config_dict, dict):
            self._fba_config = FBAConfig(**fba_config_dict)
        else:
            self._fba_config = FBAConfig(
                default_objective=getattr(fba_config_dict, "default_objective", "biomass_reaction"),
                solver=getattr(fba_config_dict, "solver", "glpk"),
                tolerance=getattr(fba_config_dict, "tolerance", 1e-6),
                additional_constraints=getattr(fba_config_dict, "additional_constraints", {}),
                simulation_method=getattr(fba_config_dict, "simulation_method", "pfba")
            )
        self._utils = ModelUtils()

    @property
    def fba_config(self) -> FBAConfig:
        """Get the FBA configuration"""
        return self._fba_config

    def validate_input(self, model_path: str) -> None:
        if not isinstance(model_path, str):
            raise ValueError("Model path must be a string")
        if not model_path.endswith(('.xml', '.sbml')):
            raise ValueError("Model file must be in SBML format (.xml or .sbml)")

    def _run(self, input_data: Any) -> ToolResult:
        try:
            # Support both dict and string inputs
            if isinstance(input_data, dict):
                model_path = input_data.get("model_path")
                output_dir = input_data.get("output_dir", None)
            else:
                model_path = input_data
                output_dir = None

            self.validate_input(model_path)
            model = self._utils.load_model(model_path)

            model.solver = self.fba_config.solver
            model.tolerance = self.fba_config.tolerance

            # Set objective if available
            if hasattr(model, self.fba_config.default_objective):
                model.objective = self.fba_config.default_objective

            # Apply additional constraints if specified
            for reaction_id, bound in self.fba_config.additional_constraints.items():
                if reaction_id in model.reactions:
                    model.reactions.get_by_id(reaction_id).bounds = (-bound, bound)

            # Run simulation using the specified method (FBA, pfba, geometric, or slim)
            simulation_method = self.fba_config.simulation_method
            solution = run_simulation(model, method=simulation_method)

            if solution.status != 'optimal':
                return ToolResult(
                    success=False,
                    message="FBA simulation failed to produce an optimal solution",
                    error=f"Solution status: {solution.status}"
                )

            significant_fluxes = {
                rxn.id: float(solution.fluxes[rxn.id])
                for rxn in model.reactions
                if abs(solution.fluxes[rxn.id]) > self.fba_config.tolerance
            }

            subsystem_fluxes = {}
            for rxn_id, flux in significant_fluxes.items():
                rxn = model.reactions.get_by_id(rxn_id)
                subsystem = rxn.subsystem or "Unknown"
                if subsystem not in subsystem_fluxes:
                    subsystem_fluxes[subsystem] = []
                subsystem_fluxes[subsystem].append((rxn_id, flux))

            # Export simulation results if an output directory is provided
            result_file = None
            if output_dir:
                print(f"DEBUG-FBA: Attempting to save results to {output_dir}")
                store = SimulationResultsStore()
                result_id = store.save_results(self.tool_name, model.id, solution, additional_metadata={"simulation_method": simulation_method})
                json_path, csv_path = store.export_results(result_id, output_dir)
                result_file = {"json": json_path, "csv": csv_path}

            return ToolResult(
                success=True,
                message="FBA simulation completed successfully",
                data={
                    "objective_value": solution.objective_value,
                    "status": solution.status,
                    "significant_fluxes": significant_fluxes,
                    "subsystem_fluxes": subsystem_fluxes,
                    "result_file": result_file,
                },
                metadata={
                    "model_id": model.id,
                    "objective_reaction": str(model.objective.expression),
                    "num_reactions": len(model.reactions),
                    "num_metabolites": len(model.metabolites),
                    "num_significant_fluxes": len(significant_fluxes)
                }
            )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error running FBA simulation",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/cobra/fba.py
################################################################################

================================================================================
FILE: src/tools/cobra/minimal_media.py
================================================================================

from typing import Dict, Any
from pydantic import BaseModel, Field, PrivateAttr
import cobra
from ..base import BaseTool, ToolResult, ToolRegistry
from .utils import ModelUtils
from .simulation_wrapper import run_simulation
from .fba import SimulationResultsStore  # Import the results store

class MinimalMediaConfig(BaseModel):
    """Configuration for minimal media finder tool."""
    growth_threshold: float = 1e-6

@ToolRegistry.register
class MinimalMediaTool(BaseTool):
    """Tool to determine the minimal set of media components required for growth using standard FBA."""
    tool_name = "find_minimal_media"
    tool_description = "Determine the minimal set of media components required for growth by iteratively removing nutrients using FBA."

    _config: MinimalMediaConfig

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        config_dict = config.get("minimal_media_config", {})
        self._config = MinimalMediaConfig(**config_dict)
        self._utils = ModelUtils()

    def _run(self, input_data: Any) -> ToolResult:
        try:
            # Allow input to be a dict with "model_path" and optional "output_dir"
            if isinstance(input_data, dict):
                model_path = input_data.get("model_path")
                output_dir = input_data.get("output_dir", None)
            else:
                model_path = input_data
                output_dir = None

            model = self._utils.load_model(model_path)
            complete_solution = run_simulation(model, method="fba")
            if complete_solution.objective_value < self._config.growth_threshold:
                return ToolResult(
                    success=False,
                    message="Complete media does not support growth.",
                    error="No growth with provided media."
                )

            # Optionally export the complete simulation results
            result_file = None
            if output_dir:
                store = SimulationResultsStore()
                result_id = store.save_results(self.tool_name, model.id, complete_solution, additional_metadata={"simulation_method": "fba"})
                json_path, csv_path = store.export_results(result_id, output_dir)
                result_file = {"json": json_path, "csv": csv_path}

            minimal_media = {}
            for rxn in model.exchanges:
                original_bounds = rxn.bounds
                rxn.lower_bound = 0
                test_solution = run_simulation(model, method="fba")
                if test_solution.objective_value < self._config.growth_threshold:
                    minimal_media[rxn.id] = original_bounds
                else:
                    minimal_media[rxn.id] = (0, original_bounds[1])
                rxn.bounds = original_bounds
            return ToolResult(
                success=True,
                message="Minimal media determination completed.",
                data={
                    "minimal_media": minimal_media,
                    "complete_solution": complete_solution.objective_value,
                    "result_file": result_file
                }
            )
        except Exception as e:
            return ToolResult(
                success=False,
                message="Error determining minimal media.",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/cobra/minimal_media.py
################################################################################

================================================================================
FILE: src/tools/cobra/missing_media.py
================================================================================

from typing import Dict, Any
from pydantic import BaseModel, Field
import cobra
from ..base import BaseTool, ToolResult, ToolRegistry
from .utils import ModelUtils
from .simulation_wrapper import run_simulation
from .fba import SimulationResultsStore  # Import the results store

class MissingMediaConfig(BaseModel):
    """Configuration for missing media analysis tool."""
    growth_threshold: float = 1e-6
    essential_metabolites: list = Field(default_factory=lambda: [
        "EX_glc__D_e", "EX_o2_e", "EX_nh4_e", "EX_pi_e", "EX_so4_e"
    ])
    supplementation_amount: float = 10.0

@ToolRegistry.register
class MissingMediaTool(BaseTool):
    """Tool to check for missing media components using standard FBA."""
    tool_name = "check_missing_media"
    tool_description = "Identify missing media components in a metabolic model using FBA."

    _config: MissingMediaConfig

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        config_dict = config.get("missing_media_config", {})
        self._config = MissingMediaConfig(**config_dict)
        self._utils = ModelUtils()

    def _run(self, input_data: Any) -> ToolResult:
        try:
            if isinstance(input_data, dict):
                model_path = input_data.get("model_path")
                output_dir = input_data.get("output_dir", None)
            else:
                model_path = input_data
                output_dir = None

            model = self._utils.load_model(model_path)
            solution = run_simulation(model, method="fba")

            # Optionally export simulation results
            result_file = None
            if output_dir:
                store = SimulationResultsStore()
                result_id = store.save_results(self.tool_name, model.id, solution, additional_metadata={"simulation_method": "fba"})
                json_path, csv_path = store.export_results(result_id, output_dir)
                result_file = {"json": json_path, "csv": csv_path}

            if solution.status != 'optimal' or solution.objective_value < self._config.growth_threshold:
                missing = []
                for met in self._config.essential_metabolites:
                    try:
                        rxn = model.reactions.get_by_id(met)
                        original_bounds = rxn.bounds
                        rxn.lower_bound = -self._config.supplementation_amount
                        test_solution = run_simulation(model, method="fba")
                        rxn.bounds = original_bounds
                        if test_solution.objective_value >= self._config.growth_threshold:
                            missing.append(met)
                    except Exception:
                        continue
                return ToolResult(
                    success=True,
                    message="Missing media components identified.",
                    data={"missing_components": missing, "objective_value": solution.objective_value, "result_file": result_file}
                )
            else:
                return ToolResult(
                    success=True,
                    message="Model grows under current media. No missing components detected.",
                    data={"objective_value": solution.objective_value, "result_file": result_file}
                )
        except Exception as e:
            return ToolResult(
                success=False,
                message="Error checking missing media components.",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/cobra/missing_media.py
################################################################################

================================================================================
FILE: src/tools/cobra/reaction_expression.py
================================================================================

from typing import Dict, Any
from pydantic import BaseModel, Field, PrivateAttr
import cobra
from ..base import BaseTool, ToolResult, ToolRegistry
from .utils import ModelUtils
from .simulation_wrapper import run_simulation
from .fba import SimulationResultsStore  # Import the results store

class ReactionExpressionConfig(BaseModel):
    """Configuration for reaction expression analysis tool."""
    flux_threshold: float = 1e-6

@ToolRegistry.register
class ReactionExpressionTool(BaseTool):
    """Tool to analyze reaction expression levels using parsimonious FBA (pFBA)."""
    tool_name = "analyze_reaction_expression"
    tool_description = "Analyze reaction expression levels using pFBA to obtain realistic flux distributions."

    _config: ReactionExpressionConfig

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        config_dict = config.get("reaction_expression_config", {})
        self._config = ReactionExpressionConfig(**config_dict)
        self._utils = ModelUtils()

    def _run(self, input_data: Any, media: dict = None) -> ToolResult:
        try:
            if isinstance(input_data, dict):
                model_path = input_data.get("model_path")
                output_dir = input_data.get("output_dir", None)
            else:
                model_path = input_data
                output_dir = None

            model = self._utils.load_model(model_path)
            if media:
                # Apply media conditions if such a method exists in ModelUtils.
                ModelUtils.apply_media_conditions(model, media)
            solution = run_simulation(model, method="pfba")

            # Optionally export simulation results
            result_file = None
            if output_dir:
                store = SimulationResultsStore()
                result_id = store.save_results(self.tool_name, model.id, solution, additional_metadata={"simulation_method": "pfba", "media": media})
                json_path, csv_path = store.export_results(result_id, output_dir)
                result_file = {"json": json_path, "csv": csv_path}

            if solution.status != 'optimal':
                return ToolResult(
                    success=False,
                    message="pFBA optimization failed",
                    error=f"Solution status: {solution.status}"
                )
            active_reactions = {
                rxn.id: float(solution.fluxes[rxn.id])
                for rxn in model.reactions
                if abs(solution.fluxes[rxn.id]) > self._config.flux_threshold
            }
            return ToolResult(
                success=True,
                message="Reaction expression analysis completed successfully",
                data={"active_reactions": active_reactions, "objective_value": solution.objective_value, "result_file": result_file},
                metadata={"num_active_reactions": len(active_reactions)}
            )
        except Exception as e:
            return ToolResult(
                success=False,
                message="Error analyzing reaction expression",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/cobra/reaction_expression.py
################################################################################

================================================================================
FILE: src/tools/cobra/simulation_wrapper.py
================================================================================

import cobra
from cobra.flux_analysis import pfba, geometric_fba, flux_variability_analysis

def run_simulation(model, method="fba", **kwargs):
    """
    Run a metabolic simulation on a COBRApy model using the specified method.

    Parameters:
      model : cobra.Model
          The metabolic model.
      method : str
          Simulation method to use. Supported options:
            - "fba": Standard Flux Balance Analysis (model.optimize())
            - "pfba": Parsimonious FBA (cobra.flux_analysis.pfba(model, **kwargs))
            - "geometric": Geometric FBA (cobra.flux_analysis.geometric_fba(model, **kwargs))
            - "slim": Slim optimization (model.slim_optimize(**kwargs))
                     [Note: returns only the objective value]
            - "fva": Flux Variability Analysis (cobra.flux_analysis.flux_variability_analysis(model, **kwargs))
      **kwargs:
          Additional arguments to pass to the simulation function.

    Returns:
      The simulation result. For "fba", "pfba", and "geometric", a cobra.Solution object is returned.
      For "slim", a float (objective value) is returned.
      For "fva", a pandas DataFrame is returned.
    """
    method = method.lower()
    if method == "pfba":
        solution = pfba(model, **kwargs)
    elif method == "geometric":
        solution = geometric_fba(model, **kwargs)
    elif method == "slim":
        solution = model.slim_optimize(**kwargs)
    elif method == "fva":
        solution = flux_variability_analysis(model, **kwargs)
    elif method == "fba":
        solution = model.optimize(**kwargs)
    else:
        raise ValueError(f"Unsupported simulation method '{method}'. Supported methods: 'fba', 'pfba', 'geometric', 'slim', 'fva'.")

    return solution

################################################################################
END OF FILE: src/tools/cobra/simulation_wrapper.py
################################################################################

================================================================================
FILE: src/tools/cobra/utils.py
================================================================================

from typing import Dict, Any, Optional, Union
import cobra
from cobra.io import read_sbml_model, write_sbml_model
import os
import logging
import tempfile
from pathlib import Path

logger = logging.getLogger(__name__)

class ModelUtils:
    """Utility functions for working with COBRA models"""

    @staticmethod
    def load_model(model_path: str) -> cobra.Model:
        """
        Load a metabolic model from a file.

        Args:
            model_path: Path to the SBML model file

        Returns:
            cobra.Model: The loaded metabolic model

        Raises:
            FileNotFoundError: If the model file doesn't exist
            ValueError: If the model file is invalid
        """
        try:
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"Model file not found: {model_path}")

            model = read_sbml_model(model_path)
            logger.info(f"Successfully loaded model: {model.id}")
            return model

        except Exception as e:
            logger.error(f"Error loading model from {model_path}: {str(e)}")
            raise ValueError(f"Failed to load model: {str(e)}")

    @staticmethod
    def save_model(model: cobra.Model, output_path: str) -> str:
        """
        Save a metabolic model to a file.

        Args:
            model: The metabolic model to save
            output_path: Path where to save the model

        Returns:
            str: Path to the saved model file

        Raises:
            ValueError: If the model cannot be saved
        """
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            # Save model
            write_sbml_model(model, output_path)
            logger.info(f"Successfully saved model to: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"Error saving model to {output_path}: {str(e)}")
            raise ValueError(f"Failed to save model: {str(e)}")

    @staticmethod
    def verify_model(model: cobra.Model) -> Dict[str, Any]:
        """
        Verify model integrity and return basic statistics.

        Args:
            model: The metabolic model to verify

        Returns:
            Dict containing model verification results
        """
        verification = {
            "is_valid": True,
            "issues": [],
            "statistics": {
                "reactions": len(model.reactions),
                "metabolites": len(model.metabolites),
                "genes": len(model.genes)
            }
        }

        # Check for isolated metabolites
        isolated = []
        for met in model.metabolites:
            if len(met.reactions) == 0:
                isolated.append(met.id)
        if isolated:
            verification["issues"].append({
                "type": "isolated_metabolites",
                "description": "Metabolites not connected to any reactions",
                "items": isolated
            })

        # Check for reactions without genes
        no_genes = []
        for rxn in model.reactions:
            if len(rxn.genes) == 0 and not rxn.id.startswith("EX_"):
                no_genes.append(rxn.id)
        if no_genes:
            verification["issues"].append({
                "type": "no_gene_association",
                "description": "Non-exchange reactions without gene associations",
                "items": no_genes
            })

        # Check mass balance
        unbalanced = []
        for reaction in model.reactions:
            if not reaction.id.startswith("EX_") and not reaction.check_mass_balance():
                unbalanced.append(reaction.id)
        if unbalanced:
            verification["issues"].append({
                "type": "unbalanced_reactions",
                "description": "Reactions with unbalanced mass",
                "items": unbalanced
            })

        # Update validity flag if issues were found
        if verification["issues"]:
            verification["is_valid"] = False

        return verification

    @staticmethod
    def get_reaction_info(model: cobra.Model, reaction_id: str) -> Dict[str, Any]:
        """
        Get detailed information about a specific reaction.

        Args:
            model: The metabolic model
            reaction_id: ID of the reaction to analyze

        Returns:
            Dict containing reaction information

        Raises:
            ValueError: If reaction is not found in the model
        """
        try:
            reaction = model.reactions.get_by_id(reaction_id)
            return {
                "id": reaction.id,
                "name": reaction.name,
                "subsystem": reaction.subsystem,
                "reaction_string": reaction.build_reaction_string(),
                "metabolites": {
                    met.id: coef for met, coef in reaction.metabolites.items()
                },
                "genes": [gene.id for gene in reaction.genes],
                "gene_reaction_rule": reaction.gene_reaction_rule,
                "bounds": reaction.bounds,
                "objective_coefficient": reaction.objective_coefficient,
                "is_exchange": reaction.id.startswith("EX_"),
                "is_balanced": reaction.check_mass_balance()
            }
        except KeyError:
            raise ValueError(f"Reaction not found in model: {reaction_id}")

    @staticmethod
    def get_metabolite_info(model: cobra.Model, metabolite_id: str) -> Dict[str, Any]:
        """
        Get detailed information about a specific metabolite.

        Args:
            model: The metabolic model
            metabolite_id: ID of the metabolite to analyze

        Returns:
            Dict containing metabolite information

        Raises:
            ValueError: If metabolite is not found in the model
        """
        try:
            metabolite = model.metabolites.get_by_id(metabolite_id)
            return {
                "id": metabolite.id,
                "name": metabolite.name,
                "formula": metabolite.formula if hasattr(metabolite, "formula") else None,
                "charge": metabolite.charge if hasattr(metabolite, "charge") else None,
                "compartment": metabolite.compartment if hasattr(metabolite, "compartment") else None,
                "reactions": [rxn.id for rxn in metabolite.reactions],
                "producing_reactions": [
                    rxn.id for rxn in metabolite.reactions
                    if metabolite in rxn.products
                ],
                "consuming_reactions": [
                    rxn.id for rxn in metabolite.reactions
                    if metabolite in rxn.reactants
                ]
            }
        except KeyError:
            raise ValueError(f"Metabolite not found in model: {metabolite_id}")

    @staticmethod
    def find_deadend_metabolites(model: cobra.Model) -> Dict[str, list]:
        """
        Find dead-end metabolites in the model.

        Args:
            model: The metabolic model

        Returns:
            Dict containing lists of dead-end metabolites
        """
        no_production = []
        no_consumption = []
        disconnected = []

        for metabolite in model.metabolites:
            producers = sum(1 for rxn in metabolite.reactions if metabolite in rxn.products)
            consumers = sum(1 for rxn in metabolite.reactions if metabolite in rxn.reactants)

            if producers == 0 and consumers == 0:
                disconnected.append(metabolite.id)
            elif producers == 0:
                no_production.append(metabolite.id)
            elif consumers == 0:
                no_consumption.append(metabolite.id)

        return {
            "no_production": no_production,
            "no_consumption": no_consumption,
            "disconnected": disconnected
        }

################################################################################
END OF FILE: src/tools/cobra/utils.py
################################################################################

================================================================================
FILE: src/tools/modelseed/__init__.py
================================================================================

from .builder import ModelBuildTool
from .gapfill import GapFillTool

__all__ = [
    'ModelBuildTool',
    'GapFillTool'
]

################################################################################
END OF FILE: src/tools/modelseed/__init__.py
################################################################################

================================================================================
FILE: src/tools/modelseed/builder.py
================================================================================

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from pathlib import Path
from ...tools.base import BaseTool, ToolResult, ToolRegistry

class ModelBuildConfig(BaseModel):
    """Configuration for ModelSEED model building"""
    template_model: Optional[str] = None
    media_condition: str = "Complete"
    genome_domain: str = "Bacteria"
    gapfill_on_build: bool = True
    additional_config: Dict[str, Any] = Field(default_factory=dict)

@ToolRegistry.register
class ModelBuildTool(BaseTool):
    """Tool for building metabolic models using ModelSEED"""

    name = "build_metabolic_model"
    description = """Build a metabolic model from genome annotations using ModelSEED.
    Can use RAST annotations or other supported annotation formats."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.build_config = ModelBuildConfig(**config.get("build_config", {}))

    def _run(self, input_data: Dict[str, Any]) -> ToolResult:
        """
        Build a metabolic model using ModelSEED.

        Args:
            input_data: Dictionary containing:
                - annotation_file: Path to genome annotation file
                - output_path: Where to save the model
                - model_id: Identifier for the new model
                - options: Additional building options

        Returns:
            ToolResult containing the built model information
        """
        try:
            # TODO: Implement actual ModelSEED integration
            # This is a placeholder implementation
            return ToolResult(
                success=False,
                message="Model building not yet implemented",
                error="Method not implemented"
            )

            # Example implementation structure:
            # 1. Validate inputs
            # annotation_file = Path(input_data["annotation_file"])
            # if not annotation_file.exists():
            #     raise FileNotFoundError(f"Annotation file not found: {annotation_file}")

            # 2. Initialize ModelSEED client
            # client = self._initialize_modelseed_client()

            # 3. Submit build job
            # job_id = self._submit_build_job(
            #     client,
            #     annotations=annotation_file,
            #     model_id=input_data["model_id"]
            # )

            # 4. Monitor progress
            # status = self._monitor_job(client, job_id)

            # 5. Get and save model
            # model = self._get_model(client, job_id)
            # self._save_model(model, input_data["output_path"])

            # return ToolResult(
            #     success=True,
            #     message="Model built successfully",
            #     data={
            #         "model_id": input_data["model_id"],
            #         "model_path": input_data["output_path"],
            #         "statistics": self._get_model_statistics(model)
            #     }
            # )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error building model",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/modelseed/builder.py
################################################################################

================================================================================
FILE: src/tools/modelseed/gapfill.py
================================================================================

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from pathlib import Path
from ...tools.base import BaseTool, ToolResult, ToolRegistry

class GapFillConfig(BaseModel):
    """Configuration for ModelSEED gap filling"""
    media_condition: str = "Complete"
    allow_reactions: List[str] = Field(default_factory=list)
    blacklist_reactions: List[str] = Field(default_factory=list)
    max_solutions: int = 1
    objective_function: str = "biomass"
    additional_config: Dict[str, Any] = Field(default_factory=dict)

@ToolRegistry.register
class GapFillTool(BaseTool):
    """Tool for gap filling metabolic models using ModelSEED"""

    name = "gapfill_model"
    description = """Perform gap filling on metabolic models to resolve network gaps
    and enable model growth using ModelSEED."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.gapfill_config = GapFillConfig(**config.get("gapfill_config", {}))

    def _run(self, input_data: Dict[str, Any]) -> ToolResult:
        """
        Perform gap filling on a metabolic model.

        Args:
            input_data: Dictionary containing:
                - model_path: Path to input model file
                - media_condition: Growth condition to test
                - output_path: Where to save the gapfilled model
                - options: Additional gap filling options

        Returns:
            ToolResult containing the gap filling results
        """
        try:
            # TODO: Implement actual ModelSEED gap filling
            # This is a placeholder implementation
            return ToolResult(
                success=False,
                message="Gap filling not yet implemented",
                error="Method not implemented"
            )

            # Example implementation structure:
            # 1. Load and validate model
            # model = self._load_model(input_data["model_path"])
            # self._validate_model(model)

            # 2. Initialize gap filling
            # gapfiller = self._initialize_gapfiller(
            #     model,
            #     media=input_data["media_condition"]
            # )

            # 3. Run gap filling
            # solutions = self._run_gapfilling(gapfiller)

            # 4. Apply best solution
            # gapfilled_model = self._apply_solution(model, solutions[0])

            # 5. Save and validate
            # self._save_model(gapfilled_model, input_data["output_path"])
            # validation = self._validate_gapfilled_model(gapfilled_model)

            # return ToolResult(
            #     success=True,
            #     message="Gap filling completed successfully",
            #     data={
            #         "added_reactions": solutions[0].added_reactions,
            #         "removed_reactions": solutions[0].removed_reactions,
            #         "objective_value": solutions[0].objective_value,
            #         "validation_results": validation,
            #         "statistics": self._get_gapfilling_statistics(solutions)
            #     }
            # )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error during gap filling",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/modelseed/gapfill.py
################################################################################

================================================================================
FILE: src/tools/rast/__init__.py
================================================================================

from .annotation import RastAnnotationTool, AnnotationAnalysisTool

__all__ = [
    'RastAnnotationTool',
    'AnnotationAnalysisTool'
]

################################################################################
END OF FILE: src/tools/rast/__init__.py
################################################################################

================================================================================
FILE: src/tools/rast/annotation.py
================================================================================

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
import json
from pathlib import Path
from ...tools.base import BaseTool, ToolResult, ToolRegistry

class RastAnnotationConfig(BaseModel):
    """Configuration for RAST annotation tools"""
    confidence_threshold: float = 0.5
    include_hypothetical: bool = False
    min_sequence_length: int = 150
    additional_config: Dict[str, Any] = Field(default_factory=dict)

class AnnotationResult(BaseModel):
    """Structure for RAST annotation results"""
    feature_id: str
    type: str
    function: str
    subsystem: Optional[str] = None
    confidence: float
    sequence_length: int
    location: Dict[str, Any]
    metadata: Dict[str, Any] = Field(default_factory=dict)

@ToolRegistry.register
class RastAnnotationTool(BaseTool):
    """Tool for running RAST genome annotations"""

    name = "run_rast_annotation"
    description = """Run RAST genome annotation on input sequences.
    Identifies protein-encoding genes and their functions."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.annotation_config = RastAnnotationConfig(**config.get("annotation_config", {}))

    def _run(self, input_data: Dict[str, Any]) -> ToolResult:
        """
        Run RAST annotation on input sequence data.

        Args:
            input_data: Dictionary containing:
                - sequence_file: Path to input sequence file
                - file_type: Type of sequence file (e.g., 'fasta', 'genbank')
                - options: Optional annotation parameters

        Returns:
            ToolResult containing annotation results
        """
        try:
            # TODO: Implement actual RAST API integration
            # This is a placeholder implementation
            return ToolResult(
                success=False,
                message="RAST annotation not yet implemented",
                error="Method not implemented"
            )

            # Example implementation structure:
            # 1. Validate input file
            # sequence_file = Path(input_data["sequence_file"])
            # if not sequence_file.exists():
            #     raise FileNotFoundError(f"Sequence file not found: {sequence_file}")

            # 2. Submit to RAST
            # job_id = self._submit_to_rast(sequence_file)

            # 3. Monitor job
            # status = self._monitor_job(job_id)

            # 4. Get results
            # annotations = self._get_results(job_id)

            # return ToolResult(
            #     success=True,
            #     message="Annotation completed successfully",
            #     data={
            #         "job_id": job_id,
            #         "annotations": annotations,
            #         "statistics": self._get_statistics(annotations)
            #     }
            # )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error running RAST annotation",
                error=str(e)
            )

@ToolRegistry.register
class AnnotationAnalysisTool(BaseTool):
    """Tool for analyzing RAST annotation results"""

    name = "analyze_rast_annotations"
    description = """Analyze RAST annotation results to identify metabolic functions,
    pathways, and potential modeling targets."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.annotation_config = RastAnnotationConfig(**config.get("annotation_config", {}))

    def _run(self, input_data: Dict[str, Any]) -> ToolResult:
        """
        Analyze RAST annotation results.

        Args:
            input_data: Dictionary containing:
                - annotation_file: Path to RAST annotation results
                - analysis_type: Type of analysis to perform

        Returns:
            ToolResult containing analysis results
        """
        try:
            # TODO: Implement actual annotation analysis
            # This is a placeholder implementation
            return ToolResult(
                success=False,
                message="Annotation analysis not yet implemented",
                error="Method not implemented"
            )

            # Example implementation structure:
            # 1. Load annotation data
            # annotations = self._load_annotations(input_data["annotation_file"])

            # 2. Analyze metabolic functions
            # metabolic_functions = self._analyze_metabolic_functions(annotations)

            # 3. Identify pathways
            # pathways = self._identify_pathways(annotations)

            # 4. Generate statistics
            # statistics = self._generate_statistics(annotations)

            # return ToolResult(
            #     success=True,
            #     message="Analysis completed successfully",
            #     data={
            #         "metabolic_functions": metabolic_functions,
            #         "pathways": pathways,
            #         "statistics": statistics,
            #         "modeling_targets": self._identify_modeling_targets(annotations)
            #     }
            # )

        except Exception as e:
            return ToolResult(
                success=False,
                message="Error analyzing annotations",
                error=str(e)
            )

################################################################################
END OF FILE: src/tools/rast/annotation.py
################################################################################

================================================================================
FILE: tests/__init__.py
================================================================================

import os
import sys
from pathlib import Path

# Add the src directory to the Python path for testing
src_path = Path(__file__).parent.parent / 'src'
sys.path.append(str(src_path))

# Set up test configuration
TEST_CONFIG_PATH = Path(__file__).parent / 'data' / 'test_config.yaml'
TEST_MODEL_PATH = Path(__file__).parent / 'data' / 'models' / 'test_model.xml'
TEST_PROMPTS_DIR = Path(__file__).parent / 'data' / 'prompts'

################################################################################
END OF FILE: tests/__init__.py
################################################################################

================================================================================
FILE: tests/test_agents.py
================================================================================

import pytest
from unittest.mock import Mock, patch
from src.agents.base import BaseAgent, AgentResult
from src.agents.metabolic import MetabolicAgent
from src.agents.factory import AgentFactory
from src.llm.base import BaseLLM
from src.tools.base import BaseTool, ToolResult
from langchain.agents import AgentExecutor

@pytest.fixture
def mock_llm():
    llm = Mock(spec=BaseLLM)
    llm._generate_response.return_value.text = "Test response"
    return llm

@pytest.fixture
def mock_tool():
    tool = Mock(spec=BaseTool)
    tool.name = "test_tool"
    tool.description = "Test tool"
    tool._run.return_value = ToolResult(
        success=True,
        message="Tool executed successfully",
        data={"test": "data"}
    )
    return tool

@pytest.fixture
def mock_agent_config():
    return {
        "name": "test_agent",
        "description": "Test agent",
        "max_iterations": 3,
        "verbose": True,
        "handle_parsing_errors": True
    }

class TestBaseAgent:
    def test_init(self, mock_llm, mock_tool, mock_agent_config):
        class TestAgent(BaseAgent):
            def _create_prompt(self):
                return Mock()

            def _create_agent(self):
                return Mock()

        agent = TestAgent(mock_llm, [mock_tool], mock_agent_config)
        assert agent.llm == mock_llm
        assert len(agent.tools) == 1
        assert agent.config.name == "test_agent"

    def test_format_result(self, mock_llm, mock_tool, mock_agent_config):
        class TestAgent(BaseAgent):
            def _create_prompt(self):
                return Mock()

            def _create_agent(self):
                return Mock()

        agent = TestAgent(mock_llm, [mock_tool], mock_agent_config)
        result = agent._format_result({
            "output": "Test output",
            "intermediate_steps": [
                {"action": {"tool": "test_tool"}},
                {"action": {"tool": "test_tool"}}
            ]
        })

        assert isinstance(result, AgentResult)
        assert result.success
        assert result.message == "Test output"
        assert result.metadata["tools_used"]["test_tool"] == 2

    def test_add_remove_tool(self, mock_llm, mock_tool, mock_agent_config):
        class TestAgent(BaseAgent):
            def _create_prompt(self):
                return Mock()

            def _create_agent(self):
                return Mock()

        agent = TestAgent(mock_llm, [], mock_agent_config)
        assert len(agent.tools) == 0

        # Add tool
        agent.add_tool(mock_tool)
        assert len(agent.tools) == 1

        # Remove tool
        agent.remove_tool("test_tool")
        assert len(agent.tools) == 0

class TestMetabolicAgent:
    def test_init(self, mock_llm, mock_tool, mock_agent_config):
        agent = MetabolicAgent(mock_llm, [mock_tool], mock_agent_config)
        assert agent.llm == mock_llm
        assert len(agent.tools) == 1

    def test_create_prompt(self, mock_llm, mock_tool, mock_agent_config):
        agent = MetabolicAgent(mock_llm, [mock_tool], mock_agent_config)
        prompt = agent._create_prompt()
        assert "metabolic modeling" in prompt.template.lower()

    @patch('langchain.agents.AgentExecutor')
    def test_analyze_model(self, mock_executor, mock_llm, mock_tool, mock_agent_config):
        # Mock successful execution
        mock_executor_instance = Mock()
        mock_executor_instance.invoke.return_value = {
            "output": "Analysis complete",
            "intermediate_steps": []
        }
        mock_executor.return_value = mock_executor_instance

        agent = MetabolicAgent(mock_llm, [mock_tool], mock_agent_config)
        agent._agent_executor = mock_executor_instance

        result = agent.analyze_model("test_model.xml")
        assert result.success
        assert result.message == "Analysis complete"

        # Check that the executor was called with correct input
        mock_executor_instance.invoke.assert_called_once()
        call_args = mock_executor_instance.invoke.call_args[0][0]
        assert "test_model.xml" in call_args["input"]

    @patch('langchain.agents.AgentExecutor')
    def test_analyze_model_with_type(self, mock_executor, mock_llm, mock_tool, mock_agent_config):
        mock_executor_instance = Mock()
        mock_executor_instance.invoke.return_value = {
            "output": "Pathway analysis complete",
            "intermediate_steps": []
        }
        mock_executor.return_value = mock_executor_instance

        agent = MetabolicAgent(mock_llm, [mock_tool], mock_agent_config)
        agent._agent_executor = mock_executor_instance

        result = agent.analyze_model("test_model.xml", analysis_type="pathway")
        assert result.success
        assert result.message == "Pathway analysis complete"
        assert "pathway" in mock_executor_instance.invoke.call_args[0][0]["input"]

    @patch('langchain.agents.AgentExecutor')
    def test_suggest_improvements(self, mock_executor, mock_llm, mock_tool, mock_agent_config):
        mock_executor_instance = Mock()
        mock_executor_instance.invoke.return_value = {
            "output": "Improvement suggestions ready",
            "intermediate_steps": []
        }
        mock_executor.return_value = mock_executor_instance

        agent = MetabolicAgent(mock_llm, [mock_tool], mock_agent_config)
        agent._agent_executor = mock_executor_instance

        result = agent.suggest_improvements("test_model.xml")
        assert result.success
        assert result.message == "Improvement suggestions ready"
        assert "improvements" in mock_executor_instance.invoke.call_args[0][0]["input"].lower()

    @patch('langchain.agents.AgentExecutor')
    def test_compare_models(self, mock_executor, mock_llm, mock_tool, mock_agent_config):
        mock_executor_instance = Mock()
        mock_executor_instance.invoke.return_value = {
            "output": "Model comparison complete",
            "intermediate_steps": []
        }
        mock_executor.return_value = mock_executor_instance

        agent = MetabolicAgent(mock_llm, [mock_tool], mock_agent_config)
        agent._agent_executor = mock_executor_instance

        model_paths = ["model1.xml", "model2.xml"]
        result = agent.compare_models(model_paths)
        assert result.success
        assert result.message == "Model comparison complete"

        # Verify that both model paths are in the input
        input_text = mock_executor_instance.invoke.call_args[0][0]["input"]
        assert all(path in input_text for path in model_paths)

class TestAgentFactory:
    def test_register_agent(self):
        class CustomAgent(BaseAgent):
            def _create_prompt(self):
                return Mock()
            def _create_agent(self):
                return Mock()

        AgentFactory.register_agent("custom", CustomAgent)
        assert "custom" in AgentFactory._agent_registry

    def test_create_agent(self, mock_llm, mock_tool, mock_agent_config):
        agent = AgentFactory.create_agent(
            "metabolic",
            mock_llm,
            [mock_tool],
            mock_agent_config
        )
        assert isinstance(agent, MetabolicAgent)

    def test_create_invalid_agent(self, mock_llm, mock_tool, mock_agent_config):
        with pytest.raises(ValueError):
            AgentFactory.create_agent(
                "invalid_agent_type",
                mock_llm,
                [mock_tool],
                mock_agent_config
            )

    def test_list_available_agents(self):
        agents = AgentFactory.list_available_agents()
        assert "metabolic" in agents
        assert isinstance(agents, list)

@pytest.mark.asyncio
class TestAsyncAgent:
    async def test_arun(self, mock_llm, mock_tool, mock_agent_config):
        class TestAgent(BaseAgent):
            def _create_prompt(self):
                return Mock()
            def _create_agent(self):
                mock_executor = Mock(spec=AgentExecutor)
                mock_executor.ainvoke.return_value = {
                    "output": "Async test complete",
                    "intermediate_steps": []
                }
                return mock_executor

        agent = TestAgent(mock_llm, [mock_tool], mock_agent_config)
        result = await agent.arun({"input": "test"})
        assert result.success
        assert result.message == "Async test complete"

    async def test_arun_error(self, mock_llm, mock_tool, mock_agent_config):
        class TestAgent(BaseAgent):
            def _create_prompt(self):
                return Mock()
            def _create_agent(self):
                mock_executor = Mock(spec=AgentExecutor)
                mock_executor.ainvoke.side_effect = Exception("Async error")
                return mock_executor

        agent = TestAgent(mock_llm, [mock_tool], mock_agent_config)
        result = await agent.arun({"input": "test"})
        assert not result.success
        assert "Async error" in result.error

################################################################################
END OF FILE: tests/test_agents.py
################################################################################

================================================================================
FILE: tests/test_llm.py
================================================================================

import pytest
from unittest.mock import Mock, patch
from src.llm import BaseLLM, ArgoLLM, OpenAILLM, LocalLLM, LLMFactory
from src.llm.base import LLMResponse, LLMConfig

@pytest.fixture
def mock_config():
    return {
        "model_name": "test-model",
        "system_content": "test system content",
        "max_tokens": 100,
        "temperature": 0.7,
        "safety_settings": {
            "enabled": True,
            "max_api_calls": 10,
            "max_tokens": 1000
        }
    }

@pytest.fixture
def mock_argo_config(mock_config):
    return {
        **mock_config,
        "api_base": "https://test.api/",
        "user": "test_user"
    }

class TestBaseLLM:
    def test_init(self, mock_config):
        llm = BaseLLM(mock_config)
        assert llm.config.model_name == "test-model"
        assert llm.total_tokens == 0
        assert llm.total_calls == 0

    def test_check_limits_success(self, mock_config):
        llm = BaseLLM(mock_config)
        llm.check_limits(100)  # Should not raise exception

    def test_check_limits_exceeded(self, mock_config):
        llm = BaseLLM(mock_config)
        llm.total_tokens = 950
        with pytest.raises(ValueError, match="Token limit exceeded"):
            llm.check_limits(100)

class TestArgoLLM:
    @patch('requests.post')
    def test_generate_response(self, mock_post, mock_argo_config):
        # Mock successful API response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "response": "test response",
            "usage": {"total_tokens": 50}
        }
        mock_post.return_value = mock_response

        llm = ArgoLLM(mock_argo_config)
        response = llm._generate_response("test prompt")

        assert isinstance(response, LLMResponse)
        assert response.text == "test response"
        assert response.tokens_used == 50
        assert response.model_name == "test-model"

    @patch('requests.post')
    def test_api_error(self, mock_post, mock_argo_config):
        # Mock API error
        mock_post.side_effect = Exception("API Error")

        llm = ArgoLLM(mock_argo_config)
        with pytest.raises(ValueError, match="API Error"):
            llm._generate_response("test prompt")

class TestOpenAILLM:
    @patch('openai.ChatCompletion.create')
    def test_generate_response(self, mock_create, mock_config):
        # Mock OpenAI API response
        mock_create.return_value = {
            "choices": [{"message": {"content": "test response"}}],
            "usage": {"total_tokens": 50}
        }

        llm = OpenAILLM(mock_config)
        response = llm._generate_response("test prompt")

        assert isinstance(response, LLMResponse)
        assert response.text == "test response"
        assert response.tokens_used == 50

class TestLocalLLM:
    @patch('transformers.AutoModelForCausalLM.from_pretrained')
    @patch('transformers.AutoTokenizer.from_pretrained')
    def test_generate_response(self, mock_tokenizer, mock_model, mock_config):
        # Mock tokenizer and model
        mock_tokenizer.return_value.encode.return_value = [[1, 2, 3]]
        mock_tokenizer.return_value.decode.return_value = "test response"
        mock_model.return_value.generate.return_value = [[1, 2, 3, 4]]

        llm = LocalLLM(mock_config)
        response = llm._generate_response("test prompt")

        assert isinstance(response, LLMResponse)
        assert response.text == "test response"

class TestLLMFactory:
    def test_create_argo(self, mock_argo_config):
        llm = LLMFactory.create("argo", mock_argo_config)
        assert isinstance(llm, ArgoLLM)

    def test_create_openai(self, mock_config):
        llm = LLMFactory.create("openai", mock_config)
        assert isinstance(llm, OpenAILLM)

    def test_create_local(self, mock_config):
        llm = LLMFactory.create("local", mock_config)
        assert isinstance(llm, LocalLLM)

    def test_invalid_backend(self, mock_config):
        with pytest.raises(ValueError, match="Unsupported LLM backend"):
            LLMFactory.create("invalid", mock_config)

################################################################################
END OF FILE: tests/test_llm.py
################################################################################

================================================================================
FILE: tests/test_tools.py
================================================================================

import pytest
from pathlib import Path
from unittest.mock import Mock, patch
import cobra
from src.tools.base import BaseTool, ToolResult, ToolRegistry
from src.tools.cobra.fba import FBATool
from src.tools.cobra.analysis import ModelAnalysisTool, PathwayAnalysisTool
from src.tools.cobra.utils import ModelUtils

@pytest.fixture
def test_model_path(tmp_path):
    # Create a simple test model
    model = cobra.Model('test_model')
    reaction = cobra.Reaction('R1')
    reaction.name = 'Test Reaction'
    reaction.lower_bound = -1000
    reaction.upper_bound = 1000
    model.add_reactions([reaction])

    # Save the model
    model_path = tmp_path / "test_model.xml"
    cobra.io.write_sbml_model(model, str(model_path))
    return str(model_path)

@pytest.fixture
def mock_tool_config():
    return {
        "name": "test_tool",
        "description": "Test tool description",
        "additional_config": {}
    }

class TestBaseTool:
    def test_init(self, mock_tool_config):
        class TestTool(BaseTool):
            def _run(self, input_data):
                return ToolResult(success=True, message="Test")

        tool = TestTool(mock_tool_config)
        assert tool.config.name == "test_tool"
        assert tool.config.description == "Test tool description"

    def test_run_success(self, mock_tool_config):
        class TestTool(BaseTool):
            def _run(self, input_data):
                return ToolResult(success=True, message="Success")

        tool = TestTool(mock_tool_config)
        result = tool.run("test input")
        assert result.success
        assert result.message == "Success"

class TestFBATool:
    def test_init(self):
        config = {
            "name": "fba_tool",
            "description": "FBA tool",
            "fba_config": {
                "default_objective": "BIOMASS",
                "solver": "glpk"
            }
        }
        tool = FBATool(config)
        assert tool.config.name == "fba_tool"
        assert tool.fba_config.default_objective == "BIOMASS"

    def test_run_fba(self, test_model_path):
        config = {
            "name": "fba_tool",
            "description": "FBA tool",
            "fba_config": {
                "default_objective": "R1",
                "solver": "glpk"
            }
        }
        tool = FBATool(config)
        result = tool._run(test_model_path)

        assert result.success
        assert "objective_value" in result.data
        assert "significant_fluxes" in result.data

    def test_invalid_model_path(self):
        tool = FBATool({"name": "fba_tool", "description": "FBA tool"})
        result = tool._run("nonexistent_model.xml")
        assert not result.success
        assert "Error" in result.message

class TestModelAnalysisTool:
    def test_init(self):
        config = {
            "name": "analysis_tool",
            "description": "Analysis tool",
            "analysis_config": {
                "flux_threshold": 1e-6
            }
        }
        tool = ModelAnalysisTool(config)
        assert tool.config.name == "analysis_tool"
        assert tool.analysis_config.flux_threshold == 1e-6

    def test_model_analysis(self, test_model_path):
        tool = ModelAnalysisTool({
            "name": "analysis_tool",
            "description": "Analysis tool"
        })
        result = tool._run(test_model_path)

        assert result.success
        assert "basic_statistics" in result.data
        assert "pathway_coverage" in result.data
        assert "potential_gaps" in result.data

class TestPathwayAnalysisTool:
    def test_pathway_analysis(self, test_model_path):
        tool = PathwayAnalysisTool({
            "name": "pathway_tool",
            "description": "Pathway tool"
        })
        result = tool._run({
            "model_path": test_model_path,
            "pathway": "Test Pathway"
        })

        assert result.success
        assert "reaction_count" in result.data
        assert "reactions" in result.data

class TestModelUtils:
    def test_load_model(self, test_model_path):
        utils = ModelUtils()
        model = utils.load_model(test_model_path)
        assert isinstance(model, cobra.Model)
        assert len(model.reactions) > 0

    def test_verify_model(self, test_model_path):
        utils = ModelUtils()
        model = utils.load_model(test_model_path)
        verification = utils.verify_model(model)

        assert "is_valid" in verification
        assert "statistics" in verification
        assert "issues" in verification

    def test_find_deadend_metabolites(self, test_model_path):
        utils = ModelUtils()
        model = utils.load_model(test_model_path)
        deadends = utils.find_deadend_metabolites(model)

        assert "no_production" in deadends
        assert "no_consumption" in deadends
        assert "disconnected" in deadends

class TestToolRegistry:
    def test_register_tool(self):
        @ToolRegistry.register
        class CustomTool(BaseTool):
            name = "custom_tool"
            def _run(self, input_data):
                return ToolResult(success=True, message="Test")

        assert "custom_tool" in ToolRegistry._tools

    def test_get_tool(self):
        tool_class = ToolRegistry.get_tool("run_metabolic_fba")
        assert tool_class is not None
        assert issubclass(tool_class, BaseTool)

    def test_create_tool(self):
        tool = ToolRegistry.create_tool(
            "run_metabolic_fba",
            {"name": "fba", "description": "FBA tool"}
        )
        assert isinstance(tool, FBATool)

################################################################################
END OF FILE: tests/test_tools.py
################################################################################
