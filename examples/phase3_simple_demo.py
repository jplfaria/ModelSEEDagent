"""
Phase 3 Simple Demonstration for ModelSEEDagent

Simplified demonstration of Phase 3 Intelligence Enhancement capabilities
without external dependencies, focusing on core quality validation features.
"""

import json
import time
from datetime import datetime


def create_simple_reasoning_trace(
    trace_id, query, conclusion, tools_used, quality_level="medium"
):
    """Create a simple reasoning trace for demonstration"""

    quality_conclusions = {
        "high": """Based on comprehensive flux balance analysis using COBRA methodology, E. coli demonstrates robust growth on glucose minimal media with a predicted growth rate of 0.847 Â± 0.023 hâ»Â¹. Essential gene analysis using single gene deletion reveals 312 genes critical for growth, representing 14.2% of the genome, which aligns with established literature values from Reed et al. (2003) and Feist et al. (2007). The high growth rate indicates efficient glucose utilization through glycolysis (8.2 mmol/gDW/h) and TCA cycle (4.1 mmol/gDW/h) with balanced ATP/NADH production. Media composition analysis confirms adequate concentrations of all essential nutrients including nitrogen (NH3), phosphorus (PO4), sulfur (SO4), and trace elements (Mg2+, Fe2+). Cross-validation with experimental data demonstrates 92% agreement, providing high confidence in these predictions. The organism exhibits metabolic flexibility with alternative pathway activation under constraint conditions.""",
        "medium": """The FBA analysis shows that E. coli can grow on glucose minimal media with a growth rate of about 0.6 hâ»Â¹. Essential gene analysis indicates that several hundred genes are required for growth, which is typical for bacteria. The media contains the necessary nutrients including carbon, nitrogen, and phosphorus sources. The results suggest this is a reasonable growth scenario for the organism.""",
        "low": """The organism grows fine on the media. Results look good and everything works as expected. Growth rate seems normal.""",
        "biased": """As expected, the organism shows excellent growth performance, confirming our initial hypothesis. The results support our prediction of high metabolic efficiency and validate our approach. This confirms the anticipated growth patterns and supports our expected outcomes.""",
    }

    return {
        "trace_id": trace_id,
        "query": query,
        "steps": [
            {"step": i + 1, "tool": tool, "action": f"Execute {tool}"}
            for i, tool in enumerate(tools_used)
        ],
        "tools_used": tools_used,
        "final_conclusion": quality_conclusions.get(
            quality_level, quality_conclusions["medium"]
        ),
        "confidence_claims": (
            [{"claim": "Growth prediction", "level": 0.85}]
            if quality_level != "low"
            else []
        ),
        "evidence_citations": (
            ["FBA results", "Literature data"] if quality_level != "low" else []
        ),
        "duration": {"high": 4.2, "medium": 2.1, "low": 0.8, "biased": 1.5}.get(
            quality_level, 2.0
        ),
        "timestamp": datetime.now().isoformat(),
    }


def demonstrate_quality_dimensions():
    """Demonstrate the 5-dimensional quality assessment system"""
    print("\nðŸ” Phase 3 Quality Dimensions Assessment")
    print("=" * 50)

    # Create sample traces with different quality characteristics
    traces = [
        create_simple_reasoning_trace(
            "trace_001",
            "Analyze E. coli growth capabilities",
            "high_quality_conclusion",
            ["run_metabolic_fba", "analyze_essentiality", "validate_results"],
            "high",
        ),
        create_simple_reasoning_trace(
            "trace_002",
            "Quick growth analysis",
            "medium_quality_conclusion",
            ["run_metabolic_fba", "basic_analysis"],
            "medium",
        ),
        create_simple_reasoning_trace(
            "trace_003",
            "Simple check",
            "low_quality_conclusion",
            ["run_metabolic_fba"],
            "low",
        ),
    ]

    print(f"\nðŸ“Š Analyzing {len(traces)} reasoning traces...")

    # Simulate quality assessment for each dimension
    # quality_dimensions = [
    #     "biological_accuracy",
    #     "reasoning_transparency",
    #     "synthesis_effectiveness",
    #     "confidence_calibration",
    #     "methodological_rigor",
    # ]

    results = []

    for trace in traces:
        print(f"\nðŸ§ª Trace: {trace['trace_id']}")
        print(f"   Query: {trace['query']}")

        # Simulate dimension scoring based on trace characteristics
        conclusion_length = len(trace["final_conclusion"])
        tools_count = len(trace["tools_used"])
        evidence_count = len(trace["evidence_citations"])
        confidence_count = len(trace["confidence_claims"])

        # Calculate simulated scores
        bio_accuracy = min(1.0, (conclusion_length / 500 + evidence_count / 3) / 2)
        transparency = min(1.0, (conclusion_length / 400 + len(trace["steps"]) / 5) / 2)
        synthesis = min(1.0, tools_count / 3)
        confidence_cal = min(1.0, confidence_count / 2)
        methodology = min(1.0, (tools_count / 3 + len(trace["steps"]) / 5) / 2)

        dimension_scores = {
            "biological_accuracy": bio_accuracy,
            "reasoning_transparency": transparency,
            "synthesis_effectiveness": synthesis,
            "confidence_calibration": confidence_cal,
            "methodological_rigor": methodology,
        }

        # Calculate weighted overall score
        weights = {
            "biological_accuracy": 0.30,
            "reasoning_transparency": 0.25,
            "synthesis_effectiveness": 0.20,
            "confidence_calibration": 0.15,
            "methodological_rigor": 0.10,
        }

        overall_score = sum(
            dimension_scores[dim] * weights[dim] for dim in dimension_scores
        )

        # Assign grade
        if overall_score >= 0.9:
            grade = "A"
        elif overall_score >= 0.8:
            grade = "B+"
        elif overall_score >= 0.7:
            grade = "B"
        elif overall_score >= 0.6:
            grade = "C+"
        else:
            grade = "C"

        print(f"   ðŸ“ˆ Overall Score: {overall_score:.3f} (Grade: {grade})")

        # Display dimension scores with visual bars
        print("   ðŸ“ Dimension Scores:")
        for dim, score in dimension_scores.items():
            bar = "â–ˆ" * int(score * 10) + "â–‘" * (10 - int(score * 10))
            print(f"      {dim:25} {score:.3f} [{bar}]")

        results.append(
            {
                "trace_id": trace["trace_id"],
                "overall_score": overall_score,
                "grade": grade,
                "dimension_scores": dimension_scores,
            }
        )

    print(f"\nâœ… Quality assessment completed for {len(traces)} traces")
    return results


def demonstrate_composite_metrics():
    """Demonstrate composite metrics calculation"""
    print("\nðŸ“Š Phase 3 Composite Metrics Calculation")
    print("=" * 50)

    # Sample quality score scenarios
    scenarios = [
        {
            "name": "Excellence Scenario",
            "scores": {
                "biological_accuracy": 0.92,
                "reasoning_transparency": 0.89,
                "synthesis_effectiveness": 0.87,
                "confidence_calibration": 0.85,
                "methodological_rigor": 0.90,
            },
        },
        {
            "name": "Balanced Scenario",
            "scores": {
                "biological_accuracy": 0.75,
                "reasoning_transparency": 0.73,
                "synthesis_effectiveness": 0.71,
                "confidence_calibration": 0.68,
                "methodological_rigor": 0.76,
            },
        },
        {
            "name": "Inconsistent Scenario",
            "scores": {
                "biological_accuracy": 0.95,
                "reasoning_transparency": 0.45,
                "synthesis_effectiveness": 0.82,
                "confidence_calibration": 0.38,
                "methodological_rigor": 0.88,
            },
        },
    ]

    print(f"\nðŸŽ¯ Testing {len(scenarios)} composite scoring scenarios...")

    weights = {
        "biological_accuracy": 0.30,
        "reasoning_transparency": 0.25,
        "synthesis_effectiveness": 0.20,
        "confidence_calibration": 0.15,
        "methodological_rigor": 0.10,
    }

    for scenario in scenarios:
        print(f"\nðŸ“‹ Scenario: {scenario['name']}")
        scores = scenario["scores"]

        # Weighted average calculation
        weighted_score = sum(scores[dim] * weights[dim] for dim in scores)

        # Geometric mean calculation
        import math

        product = 1.0
        for dim, score in scores.items():
            weight = weights[dim]
            if score > 0:
                product *= math.pow(score, weight)
        geometric_mean = product

        # Harmonic mean calculation
        harmonic_sum = sum(weights[dim] / max(0.01, scores[dim]) for dim in scores)
        harmonic_mean = sum(weights.values()) / harmonic_sum

        # Consistency bonus calculation
        score_values = list(scores.values())
        mean_score = sum(score_values) / len(score_values)
        variance = sum((score - mean_score) ** 2 for score in score_values) / len(
            score_values
        )
        std_dev = math.sqrt(variance)
        consistency_bonus = max(0.0, (1.0 - std_dev / 0.5)) * 0.05

        # Excellence bonus calculation
        excellent_count = sum(1 for score in scores.values() if score >= 0.9)
        excellence_bonus = (
            (excellent_count / len(scores)) * 0.03 if excellent_count >= 3 else 0.0
        )

        # Penalties for critical deficiencies
        penalty = sum(max(0, 0.7 - score) * 0.1 for score in scores.values())

        # Final composite score
        composite_score = (
            weighted_score + consistency_bonus + excellence_bonus - penalty
        )
        composite_score = max(0.0, min(1.0, composite_score))

        # Assign grade
        if composite_score >= 0.95:
            grade = "A+"
        elif composite_score >= 0.90:
            grade = "A"
        elif composite_score >= 0.85:
            grade = "B+"
        elif composite_score >= 0.80:
            grade = "B"
        elif composite_score >= 0.75:
            grade = "C+"
        elif composite_score >= 0.70:
            grade = "C"
        elif composite_score >= 0.60:
            grade = "D"
        else:
            grade = "F"

        print(f"   ðŸ“Š Composite Score: {composite_score:.3f} (Grade: {grade})")
        print(f"   âš–ï¸  Weighted Average: {weighted_score:.3f}")
        print(f"   ðŸ“ Geometric Mean: {geometric_mean:.3f}")
        print(f"   ðŸŽ¯ Harmonic Mean: {harmonic_mean:.3f}")
        print(f"   ðŸ† Consistency Bonus: {consistency_bonus:.3f}")
        print(f"   â­ Excellence Bonus: {excellence_bonus:.3f}")
        print(f"   âš ï¸  Penalties: {penalty:.3f}")

    print(f"\nâœ… Composite metrics demonstration completed")


def demonstrate_bias_detection():
    """Demonstrate bias detection and diversity analysis"""
    print("\nðŸŽ­ Phase 3 Bias Detection & Diversity Analysis")
    print("=" * 50)

    # Create traces with different bias characteristics
    trace_samples = [
        {
            "trace_id": "unbiased_trace",
            "conclusion": "The comprehensive analysis reveals multiple factors affecting growth, including nutrient availability, pathway efficiency, and environmental conditions. Various analytical approaches were considered.",
            "tools": [
                "fba_analysis",
                "essentiality_check",
                "media_analysis",
                "validation_test",
            ],
            "bias_indicators": [],
        },
        {
            "trace_id": "confirmation_biased",
            "conclusion": "As expected, the results confirm our initial hypothesis and support our predictions. This validates our approach and confirms the anticipated outcomes.",
            "tools": ["fba_analysis", "confirmation_test"],
            "bias_indicators": ["as expected", "confirms", "validates", "anticipated"],
        },
        {
            "trace_id": "template_biased",
            "conclusion": "The analysis shows good results. The data indicates positive outcomes. It appears that the system works well. The findings reveal expected patterns.",
            "tools": ["fba_analysis"],
            "bias_indicators": [
                "analysis shows",
                "data indicates",
                "it appears",
                "findings reveal",
            ],
        },
        {
            "trace_id": "vocabulary_limited",
            "conclusion": "The thing works fine and the stuff looks good. Something seems to be working properly and it does things as expected.",
            "tools": ["fba_analysis"],
            "bias_indicators": ["thing", "stuff", "something", "it"],
        },
    ]

    print(f"\nðŸ” Analyzing {len(trace_samples)} traces for bias patterns...")

    bias_detection_results = []

    for trace in trace_samples:
        print(f"\nðŸ§ª Analyzing: {trace['trace_id']}")

        bias_detected = []

        # Check for confirmation bias
        confirmation_patterns = [
            "as expected",
            "confirms",
            "supports",
            "validates",
            "anticipated",
        ]
        confirmation_count = sum(
            1
            for pattern in confirmation_patterns
            if pattern in trace["conclusion"].lower()
        )
        if confirmation_count > 1:
            bias_detected.append(
                {
                    "type": "confirmation_bias",
                    "severity": "medium" if confirmation_count < 3 else "high",
                    "evidence": f"{confirmation_count} confirmation patterns found",
                }
            )

        # Check for template language bias
        template_patterns = [
            "analysis shows",
            "data indicates",
            "results reveal",
            "findings suggest",
        ]
        template_count = sum(
            1 for pattern in template_patterns if pattern in trace["conclusion"].lower()
        )
        if template_count > 2:
            bias_detected.append(
                {
                    "type": "template_bias",
                    "severity": "medium",
                    "evidence": f"{template_count} template patterns found",
                }
            )

        # Check for vocabulary limitations
        limited_vocab = ["thing", "stuff", "something", "it"]
        vocab_count = sum(
            1 for word in limited_vocab if word in trace["conclusion"].lower().split()
        )
        if vocab_count > 2:
            bias_detected.append(
                {
                    "type": "vocabulary_bias",
                    "severity": "low" if vocab_count < 4 else "medium",
                    "evidence": f"{vocab_count} limited vocabulary instances",
                }
            )

        # Check for tool selection bias (over-reliance on single tool)
        if len(trace["tools"]) == 1 and "fba_analysis" in trace["tools"]:
            bias_detected.append(
                {
                    "type": "tool_selection_bias",
                    "severity": "low",
                    "evidence": "Single tool reliance detected",
                }
            )

        # Calculate diversity metrics
        conclusion_words = trace["conclusion"].lower().split()
        unique_words = len(set(conclusion_words))
        total_words = len(conclusion_words)
        vocab_diversity = unique_words / max(1, total_words)

        tool_diversity = len(set(trace["tools"])) / max(1, len(trace["tools"]))

        overall_diversity = (vocab_diversity + tool_diversity) / 2

        # Assign diversity grade
        if overall_diversity >= 0.8:
            diversity_grade = "A"
        elif overall_diversity >= 0.6:
            diversity_grade = "B"
        elif overall_diversity >= 0.4:
            diversity_grade = "C"
        else:
            diversity_grade = "D"

        bias_risk = (
            "high"
            if len(bias_detected) >= 2
            else "medium" if len(bias_detected) == 1 else "low"
        )

        print(f"   ðŸŽ­ Bias Patterns: {len(bias_detected)} detected")
        for bias in bias_detected:
            severity_icon = (
                "ðŸ”´"
                if bias["severity"] == "high"
                else "ðŸŸ¡" if bias["severity"] == "medium" else "ðŸŸ¢"
            )
            print(f"      {severity_icon} {bias['type']}: {bias['evidence']}")

        print(f"   ðŸ“Š Vocabulary Diversity: {vocab_diversity:.3f}")
        print(f"   ðŸ”§ Tool Diversity: {tool_diversity:.3f}")
        print(
            f"   ðŸ† Overall Diversity: {overall_diversity:.3f} (Grade: {diversity_grade})"
        )
        print(f"   âš ï¸  Bias Risk Level: {bias_risk}")

        bias_detection_results.append(
            {
                "trace_id": trace["trace_id"],
                "bias_patterns": bias_detected,
                "diversity_score": overall_diversity,
                "diversity_grade": diversity_grade,
                "bias_risk": bias_risk,
            }
        )

    print(f"\nâœ… Bias detection and diversity analysis completed")
    return bias_detection_results


def demonstrate_integrated_validation():
    """Demonstrate integrated validation workflow"""
    print("\nðŸ”— Phase 3 Integrated Validation Workflow")
    print("=" * 50)

    print(f"\nðŸ—ï¸ Simulating integrated quality validation system...")

    # Simulate quality-aware prompt generation
    print(f"\nðŸ’¬ Step 1: Quality-Aware Prompt Generation")
    print(f"   ðŸ“ Base prompt enhanced with quality guidance")
    print(f"   ðŸ§  Context enhancement from Phase 2 applied")
    print(f"   ðŸ” Quality validation hooks installed")
    print(f"   âœ… Quality-aware prompt generated")

    # Simulate reasoning execution
    print(f"\nðŸ¤– Step 2: Quality-Monitored Reasoning Execution")
    sample_trace = create_simple_reasoning_trace(
        "integrated_demo",
        "Comprehensive metabolic analysis with quality monitoring",
        "high_quality_analysis",
        [
            "run_metabolic_fba",
            "analyze_essentiality",
            "validate_media",
            "cross_validate",
        ],
        "high",
    )
    print(f"   ðŸ§ª Reasoning trace generated: {sample_trace['trace_id']}")
    print(f"   â±ï¸  Execution time: {sample_trace['duration']}s")
    print(f"   ðŸ”§ Tools used: {len(sample_trace['tools_used'])}")

    # Simulate real-time quality monitoring
    print(f"\nðŸ“Š Step 3: Real-Time Quality Assessment")

    # Calculate quality scores
    conclusion_length = len(sample_trace["final_conclusion"])
    evidence_quality = len(sample_trace["evidence_citations"]) / 3.0
    tool_diversity = len(sample_trace["tools_used"]) / 4.0

    simulated_scores = {
        "biological_accuracy": min(
            1.0, (conclusion_length / 800 + evidence_quality) / 2
        ),
        "reasoning_transparency": min(1.0, conclusion_length / 600),
        "synthesis_effectiveness": min(1.0, tool_diversity),
        "confidence_calibration": 0.85,  # Based on confidence claims
        "methodological_rigor": min(1.0, len(sample_trace["steps"]) / 5.0),
    }

    weights = {
        "biological_accuracy": 0.30,
        "reasoning_transparency": 0.25,
        "synthesis_effectiveness": 0.20,
        "confidence_calibration": 0.15,
        "methodological_rigor": 0.10,
    }

    overall_score = sum(
        simulated_scores[dim] * weights[dim] for dim in simulated_scores
    )
    grade = "A" if overall_score >= 0.9 else "B+" if overall_score >= 0.85 else "B"

    print(f"   ðŸ“ˆ Quality Assessment: {overall_score:.3f} (Grade: {grade})")
    print(f"   ðŸ” Real-time monitoring: Active")
    print(f"   ðŸŽ¯ All quality thresholds: Met")

    # Simulate adaptive feedback
    print(f"\nðŸ”„ Step 4: Adaptive Feedback Generation")

    feedback_recommendations = []

    if overall_score >= 0.9:
        feedback_recommendations.append("Excellent quality - maintain current approach")
    elif overall_score >= 0.8:
        feedback_recommendations.append("Good quality - minor optimizations possible")
    else:
        feedback_recommendations.append("Quality improvement needed")

    for dim, score in simulated_scores.items():
        if score < 0.7:
            feedback_recommendations.append(
                f"Focus on improving {dim.replace('_', ' ')}"
            )

    print(f"   ðŸ’¡ Feedback generated: {len(feedback_recommendations)} recommendations")
    for i, rec in enumerate(feedback_recommendations, 1):
        print(f"      {i}. {rec}")

    # Simulate system learning and adaptation
    print(f"\nðŸ§  Step 5: System Learning & Adaptation")
    print(f"   ðŸ“š Quality patterns analyzed")
    print(f"   âš–ï¸  Adaptive weights updated")
    print(f"   ðŸŽ¯ Quality thresholds optimized")
    print(f"   ðŸ”„ Continuous improvement cycle active")

    print(f"\nâœ… Integrated validation workflow completed")

    return {
        "trace_validation": sample_trace,
        "quality_scores": simulated_scores,
        "overall_assessment": {"score": overall_score, "grade": grade},
        "feedback": feedback_recommendations,
    }


def run_phase3_simple_demo():
    """Run the complete Phase 3 simple demonstration"""
    print("ðŸ§¬" * 40)
    print("ModelSEEDagent Phase 3 Intelligence Enhancement")
    print("Reasoning Quality Validation + Composite Metrics")
    print("Simple Demonstration (No External Dependencies)")
    print("ðŸ§¬" * 40)

    start_time = time.time()

    try:
        # 1. Quality Dimensions Assessment
        print("\nðŸ” Component 1: Quality Dimensions Assessment")
        quality_results = demonstrate_quality_dimensions()

        # 2. Composite Metrics Calculation
        print("\nðŸ“Š Component 2: Composite Metrics System")
        demonstrate_composite_metrics()

        # 3. Bias Detection & Diversity Analysis
        print("\nðŸŽ­ Component 3: Bias Detection & Diversity")
        bias_results = demonstrate_bias_detection()

        # 4. Integrated Validation Workflow
        print("\nðŸ”— Component 4: Integrated Validation System")
        integration_results = demonstrate_integrated_validation()

        total_time = time.time() - start_time

        # Summary and Results
        print("\n" + "ðŸŽ‰" * 60)
        print("PHASE 3 SIMPLE DEMONSTRATION COMPLETED")
        print("ðŸŽ‰" * 60)

        print(f"\nðŸ“Š Demonstration Summary:")
        print(f"   â±ï¸  Total Time: {total_time:.2f} seconds")
        print(f"   ðŸ” Quality Assessments: {len(quality_results)} completed")
        print(f"   ðŸ“ˆ Composite Scenarios: 3 tested")
        print(f"   ðŸŽ­ Bias Analyses: {len(bias_results)} completed")
        print(f"   ðŸ”— Integration Demo: Full workflow")

        print(f"\nðŸ† Phase 3 Core Features Demonstrated:")
        print(f"   âœ… 5-dimensional quality assessment")
        print(f"   âœ… Advanced composite scoring (weighted, geometric, harmonic)")
        print(
            f"   âœ… Multi-type bias detection (confirmation, template, vocabulary, tool)"
        )
        print(f"   âœ… Diversity analysis and scoring")
        print(f"   âœ… Real-time quality monitoring")
        print(f"   âœ… Adaptive feedback generation")
        print(f"   âœ… Integrated validation workflow")

        print(f"\nðŸ“ˆ Quality Assessment Results:")
        avg_quality = sum(r["overall_score"] for r in quality_results) / len(
            quality_results
        )
        print(f"   ðŸ“Š Average Quality Score: {avg_quality:.3f}")

        grade_distribution = {}
        for result in quality_results:
            grade = result["grade"]
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1

        print(f"   ðŸ† Grade Distribution: {dict(grade_distribution)}")

        # Bias analysis summary
        total_bias_patterns = sum(len(r["bias_patterns"]) for r in bias_results)
        avg_diversity = sum(r["diversity_score"] for r in bias_results) / len(
            bias_results
        )

        print(f"\nðŸŽ­ Bias & Diversity Summary:")
        print(f"   ðŸš¨ Total Bias Patterns Detected: {total_bias_patterns}")
        print(f"   ðŸ“Š Average Diversity Score: {avg_diversity:.3f}")

        risk_levels = [r["bias_risk"] for r in bias_results]
        risk_distribution = {
            level: risk_levels.count(level) for level in set(risk_levels)
        }
        print(f"   âš ï¸  Risk Level Distribution: {dict(risk_distribution)}")

        print(f"\nðŸš€ Phase 3 Ready for Integration with:")
        print(f"   ðŸ“ Phase 1: Prompt Registry System")
        print(f"   ðŸ§  Phase 2: Context Enhancement Framework")
        print(f"   ðŸ”„ Phase 4: Enhanced Artifact Intelligence (Next)")

        print(f"\nðŸ’¡ Next Development Opportunities:")
        print(f"   ðŸŽ¯ Fine-tune adaptive weight optimization")
        print(f"   ðŸ›¡ï¸ Expand bias detection to additional patterns")
        print(f"   ðŸ“Š Enhance real-time monitoring granularity")
        print(f"   ðŸ”— Deepen integration with Phase 1 & 2 systems")

        return {
            "demo_successful": True,
            "total_time": total_time,
            "quality_results": quality_results,
            "bias_results": bias_results,
            "integration_demo": integration_results,
            "summary_metrics": {
                "average_quality_score": avg_quality,
                "total_bias_patterns": total_bias_patterns,
                "average_diversity_score": avg_diversity,
            },
        }

    except Exception as e:
        print(f"\nâŒ PHASE 3 SIMPLE DEMONSTRATION FAILED: {e}")
        import traceback

        traceback.print_exc()
        return {"demo_successful": False, "error": str(e)}


if __name__ == "__main__":
    # Run the simple demonstration
    results = run_phase3_simple_demo()

    if results.get("demo_successful"):
        print(f"\nðŸŽ¯ Phase 3 demonstration completed successfully!")
        print(f"ðŸ’« Ready to proceed with Phase 4 development")
    else:
        print(f"\nðŸ”§ Please address issues before proceeding to Phase 4")
